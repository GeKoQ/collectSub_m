import yaml
import aiohttp
import asyncio
import os
import base64
from urllib.parse import urlparse
import re
import datetime
import sys
from glob import glob
import shutil

# ========== é…ç½® ==========
USER_AGENTS = [
    'meta/0.2.0.5.Meta',
    'v2rayN/7.15.0',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.2 Safari/605.1.15'
]

TG_DOMAINS = [
    "t.me",
    "tx.me",
    "telegram.me",
    "tgstat.com",
]

RE_URL = r"https?://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+"

CHECK_URL_LIST = [
    'sub.789.st',
    'sub.xeton.dev',
    'subconverters.com',
    'subapi.cmliussss.net',
    'url.v1.mk'
]
TARGET = 'mixed'
CHECK_NODE_URL_STR = "https://{}/sub?target={}&url={}&insert=false"

# ========== å¤šä»£ç†è®¾ç½® ==========
def get_proxy_list():
    http_list = os.getenv("HTTP_PROXY", "").split(",")
    https_list = os.getenv("HTTPS_PROXY", "").split(",")
    socks5_list = os.getenv("SOCKS5_PROXY", "").split(",")
    proxies = [p for p in http_list + https_list + socks5_list if p]
    return proxies or []

PROXY_LIST = get_proxy_list()

# ========== æ—¥å¿—ç³»ç»Ÿ ==========
def init_logger():
    os.makedirs("logs", exist_ok=True)
    log_file = os.path.join("logs", "log.txt")
    sys.stdout = Logger(sys.stdout, log_file)
    sys.stderr = Logger(sys.stderr, log_file)

class Logger:
    def __init__(self, stream, log_file):
        self.stream = stream
        self.log_file = log_file
    def write(self, message):
        self.stream.write(message)
        with open(self.log_file, "a", encoding="utf-8") as f:
            f.write(message)
    def flush(self):
        self.stream.flush()

init_logger()
print(f"\n{'='*80}\nğŸš€ å¯åŠ¨ä»»åŠ¡æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{'='*80}")

# ========== åŠ è½½é…ç½® ==========
if not os.path.exists('pool.yaml'):
    print("âš ï¸ æœªæ‰¾åˆ° pool.yamlï¼Œç¨‹åºé€€å‡ºã€‚")
    sys.exit(1)

with open('pool.yaml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

subscriptions = config.get('subscriptions', [])
tgchannels = config.get('tgchannels', [])

# ========== å¼‚å¸¸ä¿å­˜ ==========
def save_null_data(source_url, content):
    os.makedirs("pool", exist_ok=True)
    null_path = os.path.join("pool", "NULL.txt")
    try:
        with open(null_path, "a", encoding="utf-8") as f:
            f.write(f"\n{'='*80}\næ¥æº: {source_url}\nå†…å®¹ç‰‡æ®µ:\n{content[:500]}\n")
    except Exception as e:
        print(f"[é”™è¯¯] æ— æ³•å†™å…¥ NULL.txt: {e}")

# ğŸ†• æ–°å¢å‡½æ•°ï¼šå†™å…¥ pool åŒæ­¥ Day ç›®å½•
def write_to_pool_and_day(proto, lines, mode="a"):
    """
    å†™å…¥ pool æ–‡ä»¶ï¼ŒåŒæ—¶åŒæ­¥è¦†ç›– Day/å½“æ—¥æ—¥æœŸ æ–‡ä»¶
    """
    os.makedirs("pool", exist_ok=True)
    pool_path = os.path.join("pool", f"{proto}.txt")

    # --- å†™å…¥ pool æ–‡ä»¶ ---
    if mode == "w":
        with open(pool_path, "w", encoding="utf-8") as f:
            f.write("\n".join(lines) + "\n")
    else:
        with open(pool_path, "a", encoding="utf-8") as f:
            for line in lines:
                f.write(line + "\n")

    # --- åŒæ­¥å†™å…¥ Day æ–‡ä»¶ï¼ˆè¦†ç›–å†™å…¥ï¼‰---
    today_str = datetime.datetime.now().strftime("%Y-%m-%d")
    day_dir = os.path.join("Day", today_str)
    os.makedirs(day_dir, exist_ok=True)
    day_path = os.path.join(day_dir, f"{proto}.txt")

    with open(day_path, "w", encoding="utf-8") as f:
        f.write("\n".join(sorted(set(lines))) + "\n")

    print(f"ğŸ“ å·²åŒæ­¥å†™å…¥ Day/{today_str}/{proto}.txt")

# === æ¸…ç† NULL.txt ===
def clean_null_file():
    null_path = os.path.join("pool", "NULL.txt")
    if not os.path.exists(null_path):
        return
    try:
        with open(null_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
        pattern = re.compile(r'^(?!(socks5?|https?|ss|vmess|vless|trojan|hy2?|hysteria2?|tuic|anytls|sn|wireguard|shadowsocks|shadowtls)[^\s]+).*$', re.IGNORECASE)
        kept_lines = [l for l in lines if not pattern.match(l.strip())]
        with open(null_path, "w", encoding="utf-8") as f:
            f.writelines(kept_lines)
        print(f"ğŸ§¹ å·²æ¸…ç† NULL.txtï¼Œåˆ é™¤ {len(lines) - len(kept_lines)} è¡Œæ— æ•ˆå†…å®¹")
    except Exception as e:
        print(f"[é”™è¯¯] æ¸…ç† NULL.txt å¤±è´¥: {e}")

# ========== Telegram æŠ“å– ==========
async def fetch_with_proxies(session, url):
    tried_proxies = PROXY_LIST + [None]
    for proxy in tried_proxies:
        for ua in USER_AGENTS:
            try:
                headers = {"User-Agent": ua}
                async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=30), proxy=proxy) as r:
                    text = await r.text()
                    if r.status != 200:
                        print(f"âš ï¸ çŠ¶æ€ç  {r.status} ({url}) ä½¿ç”¨ä»£ç† {proxy}")
                        continue
                    if any(x in text for x in ["Just a moment", "enable JavaScript", "Cloudflare"]):
                        print(f"ğŸš« UA [{ua[:20]}] è¢« Cloudflare æ‹¦æˆª ä½¿ç”¨ä»£ç† {proxy}")
                        continue
                    return text
            except Exception as e:
                print(f"âš ï¸ è¯·æ±‚å¤±è´¥ UA[{ua[:20]}] ä»£ç† {proxy} -> {e}")
                save_null_data(url, str(e))
    return ""

# âœ… æ”¹è¿›ç‰ˆ extract_sub_links (æ”¯æŒ hy2/hysteria2)
async def extract_sub_links(session, channel):
    all_links = []
    for domain in TG_DOMAINS:
        url = f"https://{domain}/s/{channel}"
        print(f"\nğŸŒ æ­£åœ¨è®¿é—® {url}")
        html = await fetch_with_proxies(session, url)
        if not html:
            continue

        # åŒ¹é…æ‰€æœ‰ URLï¼ˆè®¢é˜…ï¼‰
        urls = re.findall(RE_URL, html)
        for u in urls:
            if re.search(r'(sub|clash|v2ray|vmess|ss|trojan|subscribe)', u, re.IGNORECASE):
                if "t.me" not in u and "cdn-telegram" not in u:
                    all_links.append(u)

        # âœ… æ£€æµ‹æ­£æ–‡ä¸­çš„èŠ‚ç‚¹å¹¶ä¿å­˜
        node_pattern = re.compile(
            r'^(socks5?|https?|ss|vmess|vless|trojan|hy2?|hysteria2?|tuic|anytls|sn|wireguard|shadowsocks|shadowtls)[^\s]+',
            re.IGNORECASE | re.MULTILINE
        )
        matches = list(re.finditer(node_pattern, html))
        if matches:
            os.makedirs("pool", exist_ok=True)
            for match in matches:
                line = match.group(0).strip()
                proto = line.split("://")[0].lower()

                # ğŸ§© ä¿®æ”¹ï¼šè°ƒç”¨åŒæ­¥å†™å…¥å‡½æ•°
                write_to_pool_and_day(proto, [line], mode="a")

            print(f"ğŸ’¾ å·²ä» {channel} æå– {len(matches)} æ¡èŠ‚ç‚¹ï¼Œä¿å­˜åˆ° pool/ ä¸ Day/ ä¸‹")

        if urls:
            print(f"ğŸ¯ åœ¨ {domain} æå–åˆ° {len(urls)} æ¡è®¢é˜…é“¾æ¥")

    return list(set(all_links))

async def process_tgchannels(session, tgchannels):
    results = await asyncio.gather(*[extract_sub_links(session, ch) for ch in tgchannels], return_exceptions=True)
    links = []
    for res in results:
        if isinstance(res, list):
            links.extend(res)
    return list(set(links))

# ========== è®¢é˜…è½¬æ¢ ==========
async def convert_sub(session, sub_url, domain):
    api_url = CHECK_NODE_URL_STR.format(domain, TARGET, sub_url)
    tried_proxies = PROXY_LIST + [None]
    for proxy in tried_proxies:
        try:
            async with session.get(api_url, timeout=aiohttp.ClientTimeout(total=100), proxy=proxy) as response:
                status = response.status
                content = (await response.text()).strip()
                if status != 200:
                    print(f"[é”™è¯¯] {api_url} è¿”å›çŠ¶æ€ç  {status} ä»£ç† {proxy}")
                    continue
                if "<html" in content.lower() or "error" in content.lower():
                    continue
                if len(content) % 4:
                    content += "=" * (4 - len(content) % 4)
                try:
                    decoded = base64.b64decode(content).decode('utf-8', errors='ignore')
                    return [line.strip() for line in decoded.splitlines() if line.strip()]
                except Exception as e:
                    print(f"[é”™è¯¯] Base64 è§£ç å¤±è´¥ {api_url} ä»£ç† {proxy} -> {e}")
                    continue
        except Exception as e:
            print(f"[é”™è¯¯] convert_sub({domain}) å‡ºé”™ ä»£ç† {proxy} -> {repr(e)}")
    save_null_data(api_url, "å…¨éƒ¨ä»£ç†è¯·æ±‚å¤±è´¥")
    return []

async def process_subscriptions(session, subscriptions):
    tasks = [convert_sub(session, sub, dom) for sub in subscriptions for dom in CHECK_URL_LIST]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    lines = []
    for r in results:
        if isinstance(r, list):
            lines.extend(r)
    return lines

# === å»é‡å‡½æ•°ï¼ˆå«åˆ é™¤ç©ºæ–‡ä»¶ï¼‰===
def deduplicate_pool_files():
    os.makedirs("pool", exist_ok=True)
    files = glob("pool/*.txt")
    for file in files:
        try:
            with open(file, "r", encoding="utf-8") as f:
                lines = {l.strip() for l in f if l.strip()}
            if not lines:
                os.remove(file)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ç©ºæ–‡ä»¶ {file}")
                continue
            with open(file, "w", encoding="utf-8") as f:
                f.write("\n".join(sorted(lines)) + "\n")
            print(f"ğŸ§© å·²å»é‡ {file} ({len(lines)} æ¡)")
        except Exception as e:
            print(f"[é”™è¯¯] å»é‡ {file} å¤±è´¥: {e}")

# ========== ä¸»æµç¨‹ ==========
async def main():
    async with aiohttp.ClientSession() as session:
        # Step 1: æŠ“å– Telegram é¢‘é“
        if tgchannels:
            print(f"\nğŸ“¡ å¼€å§‹æŠ“å– Telegram é¢‘é“ï¼ˆå…± {len(tgchannels)} ä¸ªï¼‰")
            new_links = await process_tgchannels(session, tgchannels)
            print(f"âœ… æŠ“å–å®Œæˆï¼Œå…±å‘ç° {len(new_links)} æ¡ Telegram é“¾æ¥")
        else:
            print("âš ï¸ tgchannels ä¸ºç©ºï¼Œè·³è¿‡æŠ“å–")
            new_links = []

        # Step 2: æ›´æ–° pool.yaml
        all_subs = list(set(subscriptions + new_links))
        filtered_subs, removed = [], []
        for sub in all_subs:
            if re.search(r'\b(?:[\w-]+\.)*telesco\.pe\b', sub, re.IGNORECASE):
                removed.append(sub)
                continue
            if re.search(r'\.(?:apk|apks|exe|jpg)$', sub, re.IGNORECASE):
                removed.append(sub)
                continue
            filtered_subs.append(sub)
        config["subscriptions"] = filtered_subs
        with open("pool.yaml", "w", encoding="utf-8") as f:
            yaml.safe_dump(config, f, allow_unicode=True, sort_keys=False)
        print(f"ğŸ§¾ å·²æ›´æ–° pool.yamlï¼šä¿ç•™ {len(filtered_subs)} æ¡è®¢é˜…ï¼Œè¿‡æ»¤æ‰ {len(removed)} æ¡æ— æ•ˆé“¾æ¥")

        # Step 3: è½¬æ¢è®¢é˜…
        print(f"\nğŸ”„ å¼€å§‹è½¬æ¢ {len(filtered_subs)} æ¡è®¢é˜…...")
        proxy_lines = await process_subscriptions(session, filtered_subs)
        print(f"âœ… è½¬æ¢å®Œæˆï¼Œå…±è§£æå‡º {len(proxy_lines)} æ¡èŠ‚ç‚¹")

        # Step 4: åˆ†ç±»ä¿å­˜
        os.makedirs("pool", exist_ok=True)
        proxy_dict = {}
        for line in proxy_lines:
            try:
                parsed = urlparse(line)
                if parsed.scheme:
                    proxy_dict.setdefault(parsed.scheme, []).append(line)
            except Exception as e:
                save_null_data("Invalid proxy line", f"{line}\n{e}")

        for proto, lines in proxy_dict.items():
            # ğŸ§© ä¿®æ”¹ï¼šåŒæ­¥å†™å…¥ pool å’Œ Day
            write_to_pool_and_day(proto, sorted(set(lines)), mode="w")
            print(f"ğŸ’¾ å†™å…¥ {proto}.txtï¼Œå…± {len(lines)} æ¡ï¼ˆåŒæ­¥ Day ç›®å½•ï¼‰")

        # Step 5: æ¸…ç†ä¸å»é‡
        clean_null_file()
        deduplicate_pool_files()

        print("\nâœ… å…¨éƒ¨å®Œæˆï¼æ—¥å¿—å·²ä¿å­˜åˆ° logs/log.txt")

if __name__ == "__main__":
    asyncio.run(main())
