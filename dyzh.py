#!/usr/bin/env python3
# dyzh.py
# Enhanced version of dyzh.py
# Features:
# - Local-first subscription parsing (base64 / Clash YAML / JS inline / JSON)
# - If local parsing fails, fall back to remote subscription conversion services
# - USER_AGENTS tried in order when downloading subscriptions (and when calling APIs)
# - Telegram channel scraping (as original)
# - Detailed logging to stdout, logs/log.txt and logs/error.log (GitHub Actions friendly)
# - Clash -> node link conversion supporting vmess/vless/ss/trojan/hysteria2/tuic

import yaml
import aiohttp
import asyncio
import os
import base64
from urllib.parse import urlparse, quote
import re
import datetime
import sys
import json
import traceback
from glob import glob

# ========== é…ç½® ==========
USER_AGENTS = [
    'meta/0.2.0.5.Meta',
    'v2rayN/7.15.0',
    #'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    #'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.2 Safari/605.1.15'
]

TG_DOMAINS = [
    "t.me",
    "tx.me",
    "telegram.me",
    "tgstat.com",
]

RE_URL = r"https?://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+"

CHECK_URL_LIST = [
    'sub.789.st',
    'sub.xeton.dev',
    'subconverters.com',
    'subapi.cmliussss.net',
    'url.v1.mk'
]
TARGET = 'clash'
CHECK_NODE_URL_STR = "https://{}/sub?target={}&url={}&insert=false&config=https%3A%2F%2Fraw.nameless13.com%2Fapi%2Fpublic%2Fdl%2FzKF9vFbb%2Feasy.ini"

# ========== å¤šä»£ç†è®¾ç½® ==========
def get_proxy_list():
    http_list = os.getenv("HTTP_PROXY", "").split(",")
    https_list = os.getenv("HTTPS_PROXY", "").split(",")
    socks5_list = os.getenv("SOCKS5_PROXY", "").split(",")
    proxies = [p for p in http_list + https_list + socks5_list if p]
    # env proxies may be like http://ip:port, keep as-is for aiohttp
    return proxies or []

PROXY_LIST = get_proxy_list()

# ========== æ—¥å¿—ç³»ç»Ÿ ==========
class Logger:
    def __init__(self, stream, log_file):
        self.stream = stream
        self.log_file = log_file
    def write(self, message):
        self.stream.write(message)
        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(message)
        except Exception:
            pass
    def flush(self):
        try:
            self.stream.flush()
        except Exception:
            pass

os.makedirs("logs", exist_ok=True)
log_file = os.path.join("logs", "log.txt")
error_log_file = os.path.join("logs", "error.log")
# redirect stdout/stderr to logger while preserving console output
sys.stdout = Logger(sys.stdout, log_file)
sys.stderr = Logger(sys.stderr, error_log_file)

def log(msg):
    print(msg)

def log_error(msg):
    print(msg, file=sys.stderr)

log(f"\n{'='*80}\nğŸš€ å¯åŠ¨ä»»åŠ¡æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n{'='*80}")

# ========== åŠ è½½é…ç½® ==========
if not os.path.exists('pool.yaml'):
    log_error("âš ï¸ æœªæ‰¾åˆ° pool.yamlï¼Œç¨‹åºé€€å‡ºã€‚")
    sys.exit(1)

with open('pool.yaml', 'r', encoding='utf-8') as f:
    try:
        config = yaml.safe_load(f)
    except Exception as e:
        log_error(f"âš ï¸ è§£æ pool.yaml å¤±è´¥: {e}")
        sys.exit(1)

subscriptions = config.get('subscriptions', []) or []
tgchannels = config.get('tgchannels', []) or config.get('tgchannel', []) or []

# ========== å¼‚å¸¸ä¿å­˜ ==========
def save_null_data(source_url, content):
    os.makedirs("pool", exist_ok=True)
    null_path = os.path.join("pool", "NULL.txt")
    try:
        with open(null_path, "a", encoding="utf-8") as f:
            f.write(f"\n{'='*80}\næ¥æº: {source_url}\nå†…å®¹ç‰‡æ®µ:\n{str(content)[:500]}\n")
    except Exception as e:
        log_error(f"[é”™è¯¯] æ— æ³•å†™å…¥ NULL.txt: {e}")

# === æ¸…ç† NULL.txt ===
def clean_null_file():
    null_path = os.path.join("pool", "NULL.txt")
    if not os.path.exists(null_path):
        return
    try:
        with open(null_path, "r", encoding="utf-8") as f:
            lines = f.readlines()
        pattern = re.compile(
            r'^(?!(socks5?|https?|ss|vmess|vless|trojan|hy2?|hysteria2?|tuic|anytls|sn|wireguard|shadowsocks|shadowtls)[^\s]+).*$', 
            re.IGNORECASE
        )
        kept_lines = [l for l in lines if not pattern.match(l.strip())]
        with open(null_path, "w", encoding="utf-8") as f:
            f.writelines(kept_lines)
        log(f"ğŸ§¹ å·²æ¸…ç† NULL.txtï¼Œåˆ é™¤ {len(lines) - len(kept_lines)} è¡Œæ— æ•ˆå†…å®¹")
    except Exception as e:
        log_error(f"[é”™è¯¯] æ¸…ç† NULL.txt å¤±è´¥: {e}")

# ========== HTTP è¯·æ±‚ï¼ˆæŒ‰ USER_AGENTS é¡ºåº + å¯é€‰ä»£ç†ï¼‰ ==========
async def fetch_with_ua_and_proxies(session, url, timeout=30):
    """
    æŒ‰ PROXY_LISTï¼ˆè‹¥æœ‰ï¼‰å’Œ USER_AGENTS é¡ºåºå°è¯•è¯·æ±‚ï¼Œé‡åˆ°ç¬¬ä¸€ä¸ªæˆåŠŸè¿”å›å†…å®¹å³åœæ­¢ã€‚
    è¿”å›å­—ç¬¦ä¸²å†…å®¹æˆ–ç©ºå­—ç¬¦ä¸²
    """
    tried = []
    proxies_to_try = PROXY_LIST + [None]
    for proxy in proxies_to_try:
        for ua in USER_AGENTS:
            headers = {"User-Agent": ua}
            try:
                async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=timeout), proxy=proxy) as r:
                    status = r.status
                    text = await r.text()
                    tried.append((proxy, ua, status, len(text)))
                    if status == 200 and text and len(text.strip()) > 8:
                        # basic cloudflare/string checks
                        low = text.lower()
                        if any(x in low for x in ["just a moment", "enable javascript", "cloudflare"]):
                            log(f"ğŸš« UA [{ua}] è¢« Cloudflare æ‹¦æˆª ä½¿ç”¨ä»£ç† {proxy}")
                            continue
                        log(f"âœ… [{ua}] è·å–æˆåŠŸ ({status}, {len(text)} bytes) {url} (proxy={proxy})")
                        return text
                    else:
                        log(f"âš ï¸ [{ua}] è¿”å›çŠ¶æ€ {status} / é•¿åº¦ {len(text)} for {url} (proxy={proxy})")
            except Exception as e:
                log(f"âš ï¸ [{ua}] è¯·æ±‚å¤±è´¥ ä»£ç† {proxy} -> {e}")
                # continue to next UA/proxy
    log_error(f"ğŸš« å…¨éƒ¨ UA/ä»£ç† è¯·æ±‚å¤±è´¥: {url} ; å°è¯•è®°å½•: {tried}")
    save_null_data(url, json.dumps(tried, ensure_ascii=False))
    return ""

# ========== Telegram æŠ“å–ï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰ ==========
async def extract_sub_links(session, channel):
    all_links = []
    for domain in TG_DOMAINS:
        url = f"https://{domain}/s/{channel}"
        log(f"\nğŸŒ æ­£åœ¨è®¿é—® {url}")
        html = await fetch_with_ua_and_proxies(session, url)
        if not html:
            continue
        urls = re.findall(RE_URL, html)
        for u in urls:
            if re.search(r'(sub|clash|v2ray|vmess|ss|trojan|subscribe)', u, re.IGNORECASE):
                if "t.me" not in u and "cdn-telegram" not in u:
                    all_links.append(u)
        node_pattern = re.compile(
            r'^(socks5?|https?|ss|vmess|vless|trojan|hy2?|hysteria2?|tuic|anytls|sn|wireguard|shadowsocks|shadowtls)[^\s]+',
            re.IGNORECASE | re.MULTILINE
        )
        matches = list(re.finditer(node_pattern, html))
        if matches:
            os.makedirs("pool", exist_ok=True)
            for match in matches:
                line = match.group(0).strip()
                proto = line.split("://")[0].lower() if "://" in line else 'unknown'
                file_path = os.path.join("pool", f"{proto}.txt")
                old_lines = set()
                if os.path.exists(file_path):
                    old_lines = {l.strip() for l in open(file_path, encoding="utf-8") if l.strip()}
                if line not in old_lines:
                    with open(file_path, "a", encoding="utf-8") as f:
                        f.write(line + "\n")
            log(f"ğŸ’¾ å·²ä» {channel} æå– {len(matches)} æ¡èŠ‚ç‚¹ï¼Œä¿å­˜åˆ° pool/ ä¸‹")
        if urls:
            log(f"ğŸ¯ åœ¨ {domain} æå–åˆ° {len(urls)} æ¡è®¢é˜…é“¾æ¥")
    return list(set(all_links))

async def process_tgchannels(session, tgchannels):
    results = await asyncio.gather(*[extract_sub_links(session, ch) for ch in tgchannels], return_exceptions=True)
    links = []
    for res in results:
        if isinstance(res, list):
            links.extend(res)
    return list(set(links))

# ========== è®¢é˜…è½¬æ¢ï¼ˆè¿œç¨‹ APIï¼‰ ==========
async def convert_sub(session, sub_url, domain):
    """
    è°ƒç”¨è®¢é˜…è½¬æ¢ APIï¼Œå°†è®¢é˜…è½¬ä¸ºèŠ‚ç‚¹åˆ—è¡¨
    ç°åœ¨ sub_url ä¼šå…ˆè¿›è¡Œ URL ç¼–ç å†æ‹¼å…¥ API è°ƒç”¨ä¸­
    """
    encoded_url = quote(sub_url, safe='')
    api_url = CHECK_NODE_URL_STR.format(domain, TARGET, encoded_url)
    tried_proxies = PROXY_LIST + [None]
    for proxy in tried_proxies:
        try:
            async with session.get(api_url, timeout=aiohttp.ClientTimeout(total=100), proxy=proxy) as response:
                status = response.status
                content = (await response.text()).strip()
                if status != 200:
                    log_error(f"[é”™è¯¯] {api_url} è¿”å›çŠ¶æ€ç  {status} ä»£ç† {proxy}")
                    continue
                if "<html" in content.lower() or "error" in content.lower():
                    log_error(f"[é”™è¯¯] {api_url} è¿”å›éæœŸæœ›å†…å®¹ ä»£ç† {proxy}")
                    continue
                # padding base64
                if len(content) % 4:
                    content += "=" * (4 - len(content) % 4)
                try:
                    decoded = base64.b64decode(content).decode('utf-8', errors='ignore')
                    return [line.strip() for line in decoded.splitlines() if line.strip()]
                except Exception as e:
                    log_error(f"[é”™è¯¯] Base64 è§£ç å¤±è´¥ {api_url} ä»£ç† {proxy} -> {e}")
                    continue
        except Exception as e:
            log_error(f"[é”™è¯¯] convert_sub({domain}) å‡ºé”™ ä»£ç† {proxy} -> {repr(e)}")
    save_null_data(api_url, "å…¨éƒ¨ä»£ç†è¯·æ±‚å¤±è´¥")
    return []

async def process_subscriptions_remote(session, subscriptions):
    tasks = [convert_sub(session, sub, dom) for sub in subscriptions for dom in CHECK_URL_LIST]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    lines = []
    for r in results:
        if isinstance(r, list):
            lines.extend(r)
    return lines

# ========== Clash / YAML / JS inline -> é“¾æ¥ è½¬æ¢ ==========

def is_base64_text(s: str) -> bool:
    # heuristic: long string, contains only base64 chars and possibly newlines
    s2 = s.strip().replace('\n','')
    if len(s2) < 16:
        return False
    return re.fullmatch(r'[A-Za-z0-9+/=\\n\\r]+', s2) is not None


def decode_base64_to_lines(s: str):
    data = s.strip()
    # try padding if needed
    data = data.replace('\r', '').replace('\n', '')
    try:
        if len(data) % 4:
            data += '=' * (4 - len(data) % 4)
        decoded = base64.b64decode(data)
        try:
            text = decoded.decode('utf-8', errors='ignore')
            return [l.strip() for l in text.splitlines() if l.strip()]
        except Exception:
            return []
    except Exception:
        return []


def clash_to_links(clash_yaml_content: str):
    """
    å°† Clash é…ç½®å†…å®¹ï¼ˆYAML æˆ– JS è¡Œå†…æ ¼å¼ï¼‰è½¬æ¢ä¸ºæ ‡å‡†èŠ‚ç‚¹é“¾æ¥
    æ”¯æŒ vmess / vless / trojan / ss / hysteria2 / tuic
    """
    try:
        data = yaml.safe_load(clash_yaml_content)
    except Exception as e:
        log_error(f"âš ï¸ æ— æ³•è§£æ Clash/YAML å†…å®¹: {e}")
        return []

    proxies = data.get('proxies') or data.get('Proxy') or data.get('proxy') or []
    if not proxies:
        log_error("âš ï¸ æœªæ£€æµ‹åˆ° proxies èŠ‚ç‚¹")
        return []

    links = []
    for p in proxies:
        try:
            t = str(p.get('type', '')).lower()
            name = p.get('name') or p.get('ps') or 'Unnamed'

            if t == 'vmess':
                node = {
                    'v': '2',
                    'ps': name,
                    'add': p.get('server') or p.get('addr') or p.get('host'),
                    'port': str(p.get('port') or p.get('remote') or ''),
                    'id': p.get('uuid') or p.get('id'),
                    'aid': str(p.get('alterId', p.get('aid', 0)) or 0),
                    'net': p.get('network', p.get('net', 'tcp')),
                    'type': p.get('type', ''),
                    'host': p.get('host', '') or p.get('ws-headers', {}).get('Host', ''),
                    'path': p.get('path', ''),
                    'tls': 'tls' if p.get('tls') or p.get('tls', False) else ''
                }
                # vmess é“¾æ¥éœ€è¦ base64 ç¼–ç  JSONï¼ˆè¿™é‡Œç”¨ yaml dump ä¿æŒ unicodeï¼‰
                node_json = json.dumps(node, ensure_ascii=False)
                link = 'vmess://' + base64.b64encode(node_json.encode()).decode()
                links.append(link)

            elif t == 'vless':
                params = []
                if p.get('flow'): params.append(f"flow={p['flow']}")
                if p.get('network'): params.append(f"type={p['network']}")
                if p.get('reality-opts'):
                    ro = p['reality-opts']
                    if 'public-key' in ro:
                        params.append(f"pbk={ro['public-key']}")
                    if 'short-id' in ro:
                        params.append(f"sid={ro['short-id']}")
                if p.get('servername'): params.append(f"sni={p['servername']}")
                if p.get('client-fingerprint'): params.append(f"fp={p['client-fingerprint']}")
                security = 'reality' if 'reality-opts' in p else ('tls' if p.get('tls') else 'none')
                params.append(f"security={security}")
                qs = '&'.join(params)
                link = f"vless://{p.get('uuid')}@{p.get('server')}:{p.get('port')}?{qs}#{quote(str(name))}"
                links.append(link)

            elif t == 'trojan':
                qs = []
                if p.get('flow'): qs.append(f"flow={p['flow']}")
                if p.get('sni'): qs.append(f"sni={p['sni']}")
                if p.get('alpn'): qs.append(f"alpn={p['alpn']}")
                qs.append(f"security={'tls' if p.get('tls') else 'none'}")
                q = '&'.join(qs)
                link = f"trojan://{p.get('password')}@{p.get('server')}:{p.get('port')}?{q}#{quote(str(name))}"
                links.append(link)

            elif t in ('ss', 'shadowsocks'):
                method = p.get('cipher') or p.get('method')
                password = p.get('password')
                if method and password:
                    base = base64.b64encode(f"{method}:{password}".encode()).decode()
                    link = f"ss://{base}@{p.get('server')}:{p.get('port')}#{quote(str(name))}"
                    links.append(link)

            elif t in ('hysteria2', 'hy2'):
                qs = []
                if p.get('sni'): qs.append(f"sni={p['sni']}")
                if p.get('auth'): qs.append(f"auth={p['auth']}")
                if p.get('skip-cert-verify') or p.get('insecure'):
                    qs.append("insecure=1")
                q = '&'.join(qs)
                link = f"hysteria2://{p.get('password')}@{p.get('server')}:{p.get('port')}?{q}#{quote(str(name))}"
                links.append(link)

            elif t == 'tuic':
                qs = []
                if p.get('sni'): qs.append(f"sni={p['sni']}")
                if p.get('token'): qs.append(f"token={p['token']}")
                if p.get('alpn'): qs.append(f"alpn={p['alpn']}")
                q = '&'.join(qs)
                link = f"tuic://{p.get('uuid')}:{p.get('password')}@{p.get('server')}:{p.get('port')}?{q}#{quote(str(name))}"
                links.append(link)

            else:
                log(f"âš ï¸ æœªè¯†åˆ«çš„èŠ‚ç‚¹ç±»å‹: {t} ({name})")
        except Exception as e:
            log_error(f"[é”™è¯¯] è½¬æ¢å•ä¸ª proxy å¤±è´¥: {e} \n{traceback.format_exc()}")
    return links

# ========== pool æ“ä½œ ==========

def deduplicate_pool_files():
    os.makedirs("pool", exist_ok=True)
    files = glob("pool/*.txt")
    for file in files:
        try:
            with open(file, "r", encoding="utf-8") as f:
                lines = {l.strip() for l in f if l.strip()}
            if not lines:
                os.remove(file)
                log(f"ğŸ—‘ï¸ å·²åˆ é™¤ç©ºæ–‡ä»¶ {file}")
                continue
            with open(file, "w", encoding="utf-8") as f:
                f.write("\n".join(sorted(lines)) + "\n")
            log(f"ğŸ§© å·²å»é‡ {file} ({len(lines)} æ¡)")
        except Exception as e:
            log_error(f"[é”™è¯¯] å»é‡ {file} å¤±è´¥: {e}")


def write_to_pool_and_day(proto, new_lines):
    os.makedirs("pool", exist_ok=True)
    pool_file = os.path.join("pool", f"{proto}.txt")
    old_pool_lines = set()
    if os.path.exists(pool_file):
        old_pool_lines = {l.strip() for l in open(pool_file, encoding="utf-8") if l.strip()}
    all_pool_lines = sorted(old_pool_lines | set(new_lines))
    with open(pool_file, "w", encoding="utf-8") as f:
        f.write("\n".join(all_pool_lines) + "\n")
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    day_dir = os.path.join("Day", today)
    os.makedirs(day_dir, exist_ok=True)
    day_file = os.path.join(day_dir, f"{proto}.txt")
    old_day_lines = set()
    if os.path.exists(day_file):
        old_day_lines = {l.strip() for l in open(day_file, encoding="utf-8") if l.strip()}
    new_day_lines = set(new_lines) - old_pool_lines
    all_day_lines = sorted(old_day_lines | new_day_lines)
    if new_day_lines:
        with open(day_file, "w", encoding="utf-8") as f:
            f.write("\n".join(all_day_lines) + "\n")

# ========== æœ¬åœ°è§£æé€»è¾‘ï¼ˆè®¢é˜…ä¼˜å…ˆæœ¬åœ°è§£æï¼‰ ==========

def extract_nodes_from_text(text: str):
    """å°è¯•ä»çº¯æ–‡æœ¬ä¸­æå–èŠ‚ç‚¹è¡Œï¼ˆvmess:// / vless:// / ss:// / trojan:// ...ï¼‰"""
    lines = [l.strip() for l in text.splitlines() if l.strip()]
    nodes = []
    pattern = re.compile(r'^(socks5?|https?|ss|vmess|vless|trojan|hy2?|hysteria2?|tuic|anytls|sn|wireguard|shadowsocks|shadowtls)[^\s]+', re.IGNORECASE)
    for l in lines:
        if pattern.match(l):
            nodes.append(l)
    return nodes

async def parse_subscription_content_local(content: str):
    """
    ä¼˜å…ˆå°è¯•ï¼š
    1) è¯†åˆ«å¹¶è§£ç  base64 -> nodes
    2) ä½œä¸º YAML/Clash -> clash_to_links
    3) ä½œä¸º JSON -> æå– proxies / nodes
    4) ä»æ–‡æœ¬ä¸­æŠ½å–èŠ‚ç‚¹è¡Œ
    è¿”å›èŠ‚ç‚¹åˆ—è¡¨
    """
    # 1) base64
    if is_base64_text(content):
        lines = decode_base64_to_lines(content)
        nodes = extract_nodes_from_text('\n'.join(lines))
        if nodes:
            return nodes
    # 2) YAML/Clash
    try:
        links = clash_to_links(content)
        if links:
            return links
    except Exception:
        pass
    # 3) JSON
    try:
        data = json.loads(content)
        # common panel formats: may contain 'data' -> 'nodes' or 'proxies'
        candidates = []
        if isinstance(data, dict):
            for k in ('proxies', 'nodes', 'data'):
                if k in data:
                    candidates = data[k]
                    break
        if isinstance(candidates, list) and candidates:
            # try to convert each entry to link if possible
            out = []
            for item in candidates:
                if isinstance(item, str):
                    out.append(item)
                elif isinstance(item, dict):
                    # reuse clash_to_links by creating a fake clash wrapper
                    fake = {'proxies': [item]}
                    try:
                        out.extend(clash_to_links(yaml.safe_dump(fake, allow_unicode=True)))
                    except Exception:
                        continue
            if out:
                return out
    except Exception:
        pass
    # 4) ä»æ–‡æœ¬ä¸­æ‘˜å–èŠ‚ç‚¹è¡Œ
    nodes = extract_nodes_from_text(content)
    return nodes

# ========== ä¸»æµç¨‹ï¼ˆlocal æ¨¡å¼ + fallback remoteï¼‰ ==========
async def main_local_mode():
    async with aiohttp.ClientSession() as session:
        # Step A: Telegram é¢‘é“æŠ“å–ï¼ˆå…ˆæŠ“å– TGï¼Œæ–¹ä¾¿æ–°å¢è®¢é˜…ï¼‰
        if tgchannels:
            log(f"\nğŸ“¡ å¼€å§‹æŠ“å– Telegram é¢‘é“ï¼ˆå…± {len(tgchannels)} ä¸ªï¼‰")
            new_links = await process_tgchannels(session, tgchannels)
            log(f"âœ… æŠ“å–å®Œæˆï¼Œå…±å‘ç° {len(new_links)} æ¡ Telegram é“¾æ¥")
        else:
            log("âš ï¸ tgchannels ä¸ºç©ºï¼Œè·³è¿‡æŠ“å–")
            new_links = []

        # æ›´æ–° subscriptions åˆ—è¡¨ï¼špool.yaml + æ–°æŠ“å–çš„é“¾æ¥
        all_subs = list(dict.fromkeys(subscriptions + new_links))

        # local-first parse
        local_nodes = []
        failed_subs = []
        log(f"\nğŸ“¦ å°è¯•æœ¬åœ°è§£æè®¢é˜…ï¼ˆå…± {len(all_subs)} æ¡ï¼‰")
        for sub in all_subs:
            log(f"\nğŸ”¹ å¤„ç†è®¢é˜…: {sub}")
            content = await fetch_with_ua_and_proxies(session, sub)
            if not content:
                log_error(f"âš ï¸ æœ¬åœ°ä¸‹è½½å¤±è´¥: {sub}")
                failed_subs.append(sub)
                continue
            nodes = await parse_subscription_content_local(content)
            if nodes:
                log(f"âœ… æœ¬åœ°è§£ææˆåŠŸ: {len(nodes)} æ¡èŠ‚ç‚¹ ä» {sub}")
                local_nodes.extend(nodes)
            else:
                log(f"âš ï¸ æœ¬åœ°è§£ææœªæå–åˆ°èŠ‚ç‚¹: {sub}")
                failed_subs.append(sub)

        # For failed subs, call remote conversion API (one by one)
        if failed_subs:
            log(f"\nğŸŒ ä½¿ç”¨è®¢é˜…è½¬æ¢ API å¤„ç† {len(failed_subs)} æ¡å¤±è´¥è®¢é˜…...")
            remote_nodes = await process_subscriptions_remote(session, failed_subs)
            log(f"âœ… è¿œç¨‹è½¬æ¢å®Œæˆï¼Œå…±è§£æå‡º {len(remote_nodes)} æ¡èŠ‚ç‚¹")
        else:
            remote_nodes = []

        # åˆå¹¶æ‰€æœ‰èŠ‚ç‚¹
        proxy_lines = list(dict.fromkeys(local_nodes + remote_nodes))
        log(f"\nâœ… åˆå¹¶èŠ‚ç‚¹å®Œæˆï¼Œå…± {len(proxy_lines)} æ¡ï¼ˆå»é‡å‰ï¼‰")

        # åˆ†ç±»ä¿å­˜
        proxy_dict = {}
        for line in proxy_lines:
            try:
                parsed = urlparse(line)
                if parsed.scheme:
                    proto = parsed.scheme.lower()
                    proxy_dict.setdefault(proto, []).append(line)
                else:
                    # fallback: detect vmess/vless by prefix
                    if line.startswith('vmess://'):
                        proxy_dict.setdefault('vmess', []).append(line)
                    else:
                        save_null_data('Invalid proxy line', line)
            except Exception as e:
                save_null_data('Invalid proxy line', f"{line}\n{e}")
        for proto, lines in proxy_dict.items():
            write_to_pool_and_day(proto, lines)
            log(f"ğŸ’¾ å·²å†™å…¥ {proto}.txt åˆ° pool å¹¶æ›´æ–°å½“å¤© Day æ–‡ä»¶")

        # æ¸…ç†ä¸å»é‡
        clean_null_file()
        deduplicate_pool_files()

        log("\nâœ… å…¨éƒ¨å®Œæˆï¼æ—¥å¿—å·²ä¿å­˜åˆ° logs/log.txt")

# ========== CLI: clash è½¬æ¢æ¨¡å¼ =============

def cli_clash_mode(path_or_content):
    # å¦‚æœä¼ å…¥çš„æ˜¯æ–‡ä»¶è·¯å¾„åˆ™è¯»å–æ–‡ä»¶ï¼Œå¦åˆ™å½“ä½œå­—ç¬¦ä¸²å†…å®¹
    if os.path.exists(path_or_content):
        with open(path_or_content, 'r', encoding='utf-8') as f:
            content = f.read()
    else:
        content = path_or_content
    # å¦‚æœæ˜¯ base64 å¤§ä¸²ï¼Œå…ˆ decode
    out = []
    if is_base64_text(content):
        lines = decode_base64_to_lines(content)
        out.extend(lines)
    # å°è¯• clash parse
    try:
        c2l = clash_to_links(content)
        out.extend(c2l)
    except Exception as e:
        log_error(f"[é”™è¯¯] clash parse failed: {e}")
    # å¦‚æœæ²¡æœ‰ç»“æœï¼Œå°è¯•æå–èŠ‚ç‚¹è¡Œ
    if not out:
        out = extract_nodes_from_text(content)
    # è¾“å‡º
    for line in out:
        print(line)
    log(f"âœ… å…±è¾“å‡º {len(out)} æ¡é“¾æ¥")

# ========== å…¥å£ ==========
if __name__ == '__main__':
    if len(sys.argv) >= 2 and sys.argv[1] == 'clash':
        if len(sys.argv) >= 3:
            cli_clash_mode(sys.argv[2])
        else:
            log_error('ç”¨æ³•: python dyzh.py clash config.yaml')
        sys.exit(0)

    # local æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
    if len(sys.argv) >= 2 and sys.argv[1] == 'local':
        asyncio.run(main_local_mode())
        sys.exit(0)

    # é»˜è®¤æ‰§è¡ŒåŸæœ‰ä¸»æµç¨‹ (ä¿ç•™å‘åå…¼å®¹æ€§ï¼šæŠ“ TG + è½¬æ¢è®¢é˜…å…¨éƒ¨èµ°è¿œç¨‹)
    async def main_default():
        async with aiohttp.ClientSession() as session:
            # process tgchannels
            if tgchannels:
                log(f"\nğŸ“¡ å¼€å§‹æŠ“å– Telegram é¢‘é“ï¼ˆå…± {len(tgchannels)} ä¸ªï¼‰")
                new_links = await process_tgchannels(session, tgchannels)
                log(f"âœ… æŠ“å–å®Œæˆï¼Œå…±å‘ç° {len(new_links)} æ¡ Telegram é“¾æ¥")
            else:
                new_links = []

            # update pool.yaml
            all_subs = list(set(subscriptions + new_links))
            filtered_subs, removed = [], []
            for sub in all_subs:
                if re.search(r'\b(?:[\w-]+\.)*telesco\.pe\b', sub, re.IGNORECASE):
                    removed.append(sub)
                    continue
                if re.search(r'\.(?:apk|apks|exe|jpg)$', sub, re.IGNORECASE):
                    removed.append(sub)
                    continue
                filtered_subs.append(sub)
            config['subscriptions'] = filtered_subs
            with open('pool.yaml', 'w', encoding='utf-8') as f:
                yaml.safe_dump(config, f, allow_unicode=True, sort_keys=False)
            log(f"ğŸ§¾ å·²æ›´æ–° pool.yamlï¼šä¿ç•™ {len(filtered_subs)} æ¡è®¢é˜…ï¼Œè¿‡æ»¤æ‰ {len(removed)} æ¡æ— æ•ˆé“¾æ¥")

            # remote convert all filtered_subs
            log(f"\nğŸ”„ å¼€å§‹è¿œç¨‹è½¬æ¢ {len(filtered_subs)} æ¡è®¢é˜…...")
            proxy_lines = await process_subscriptions_remote(session, filtered_subs)
            log(f"âœ… è¿œç¨‹è½¬æ¢å®Œæˆï¼Œå…±è§£æå‡º {len(proxy_lines)} æ¡èŠ‚ç‚¹")

            # classify and write
            proxy_dict = {}
            for line in proxy_lines:
                try:
                    parsed = urlparse(line)
                    if parsed.scheme:
                        proxy_dict.setdefault(parsed.scheme, []).append(line)
                except Exception as e:
                    save_null_data('Invalid proxy line', f"{line}\n{e}")
            for proto, lines in proxy_dict.items():
                write_to_pool_and_day(proto, lines)
                log(f"ğŸ’¾ å·²å†™å…¥ {proto}.txt åˆ° pool å¹¶æ›´æ–°å½“å¤© Day æ–‡ä»¶")
            clean_null_file()
            deduplicate_pool_files()
            log("\nâœ… å…¨éƒ¨å®Œæˆï¼æ—¥å¿—å·²ä¿å­˜åˆ° logs/log.txt")

    asyncio.run(main_default())
