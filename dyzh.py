import yaml
import aiohttp
import asyncio
from bs4 import BeautifulSoup
import os
import base64
from urllib.parse import urlparse
import re

# ========== å…¨å±€é…ç½® ==========

# UA é¡ºåºå°è¯•ï¼ˆä»è½»åˆ°é‡ï¼‰
USER_AGENTS = [
    'meta/0.2.0.5.Meta',
    'v2rayN/7.15.0',
    # å¸¸è§æµè§ˆå™¨ UAï¼ˆæœ€åå°è¯•ï¼‰
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_2) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.2 Safari/605.1.15'
]

# Telegram é•œåƒåŸŸåï¼ˆè‡ªåŠ¨åˆ‡æ¢ï¼‰
TG_DOMAINS = ["t.me", "telegram.me", "tgo.li", "tg.rip"]

# åŠ è½½é…ç½®æ–‡ä»¶
with open('pool.yaml', 'r') as f:
    config = yaml.safe_load(f)

subscriptions = config.get('subscriptions', [])
tgchannels = config.get('tgchannels', [])

# ========== æ–°å¢å‡½æ•°ï¼šä¿å­˜å¼‚å¸¸æ•°æ®åˆ° NULL.txt ==========
def save_null_data(source_url, content):
    """å°†å¼‚å¸¸æˆ–æ— æ•ˆè®¢é˜…æ•°æ®ä¿å­˜åˆ° pool/NULL.txt"""
    os.makedirs("pool", exist_ok=True)
    null_path = os.path.join("pool", "NULL.txt")
    try:
        with open(null_path, "a", encoding="utf-8") as f:
            f.write(f"\n{'='*80}\n")
            f.write(f"æ¥æº: {source_url}\n")
            f.write(f"å†…å®¹ç‰‡æ®µ:\n")
            f.write(content[:500] + "\n")  # ä¿å­˜å‰ 500 å­—ç¬¦
    except Exception as e:
        print(f"[é”™è¯¯] æ— æ³•å†™å…¥ NULL.txt: {e}")

# ========== æŠ“å– Telegram é¢‘é“è®¢é˜…é“¾æ¥ ==========

async def extract_sub_links(session, channel):
    """
    ä» Telegram é¢‘é“æŠ“å–è®¢é˜…é“¾æ¥
    - é¡ºåºå°è¯•å¤šä¸ª UA
    - é‡åˆ° Cloudflare æˆ–è¶…æ—¶è‡ªåŠ¨åˆ‡æ¢é•œåƒç«™
    """
    links = []

    for domain in TG_DOMAINS:
        url = f"https://{domain}/s/{channel}"
        print(f"\nğŸŒ å°è¯•è®¿é—® {url}")

        for ua in USER_AGENTS:
            headers = {'User-Agent': ua}
            try:
                async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=20)) as response:
                    if response.status != 200:
                        print(f"âš ï¸ UA [{ua[:25]}...] è¿”å›çŠ¶æ€ç : {response.status}")
                        continue

                    text = await response.text()

                    # æ£€æµ‹æ˜¯å¦è¢« Cloudflare æ‹¦æˆª
                    if "Just a moment" in text or "Cloudflare" in text or "enable JavaScript" in text:
                        print(f"ğŸš« UA [{ua[:25]}...] è¢« Cloudflare æ‹¦æˆªï¼Œå°è¯•ä¸‹ä¸€ä¸ª UA")
                        continue

                    # è°ƒè¯•è¾“å‡º
                    print(f"âœ… UA æˆåŠŸ: {ua[:60]}...")
                    print(f"ğŸ” å†…å®¹å‰ 200 å­—ç¬¦: {text[:200].replace(chr(10),' ')}")

                    # è§£æ HTML
                    soup = BeautifulSoup(text, 'html.parser')
                    messages = soup.find_all('div', class_='tgme_widget_message_text')

                    for msg in messages:
                        for a in msg.find_all('a', href=True):
                            href = a['href']
                            if re.search(r'(sub|clash|v2ray|vmess|ss|trojan|subscribe)', href, re.IGNORECASE):
                                links.append(href)

                    if links:
                        print(f"ğŸ¯ æˆåŠŸæŠ“å– {len(links)} ä¸ªè®¢é˜…é“¾æ¥")
                        return links  # æˆåŠŸåˆ™é€€å‡ºå¾ªç¯
                    else:
                        print(f"âŒ é¡µé¢åŠ è½½æˆåŠŸä½†æœªå‘ç°è®¢é˜…é“¾æ¥ï¼Œå°è¯•ä¸‹ä¸€ä¸ª UA")

            except Exception as e:
                print(f"âš ï¸ è¯·æ±‚ {url} å¤±è´¥ ({type(e).__name__}): {e}")
                save_null_data(url, str(e))  # æ–°å¢ï¼šå¼‚å¸¸ä¹Ÿä¿å­˜

        print(f"ğŸ” {domain} å°è¯•å¤±è´¥ï¼Œåˆ‡æ¢ä¸‹ä¸€ä¸ªé•œåƒåŸŸå...")

    print(f"âŒ æ‰€æœ‰é•œåƒå‡è®¿é—®å¤±è´¥: {channel}")
    return []

# å¼‚æ­¥å¤„ç† Telegram é¢‘é“
async def process_tgchannels(session, tgchannels):
    tasks = [extract_sub_links(session, channel) for channel in tgchannels]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    new_links = []
    for result in results:
        if not isinstance(result, Exception):
            new_links.extend(result)
    return new_links

# ========== è®¢é˜…è½¬æ¢éƒ¨åˆ†ï¼ˆåŸæ ·ä¿ç•™ + æ–°å¢å¼‚å¸¸ä¿å­˜ï¼‰ ==========

CHECK_URL_LIST = ['sub.789.st', 'sub.xeton.dev', 'subconverters.com', 'subapi.cmliussss.net', 'url.v1.mk']
target = 'mixed'
CHECK_NODE_URL_STR = "https://{}/sub?target={}&url={}&insert=false"

async def convert_sub(session, sub_url, domain):
    api_url = CHECK_NODE_URL_STR.format(domain, target, sub_url)
    try:
        async with session.get(api_url, timeout=aiohttp.ClientTimeout(total=100)) as response:
            if response.status == 200:
                content = await response.text()
                content = content.strip()
                # å¦‚æœè¿”å›HTMLè¯´æ˜è¢«æ‹¦æˆªæˆ–é”™è¯¯
                if "<html" in content.lower():
                    print(f"[è­¦å‘Š] {api_url} è¿”å› HTMLï¼Œç–‘ä¼¼æ‹¦æˆªæˆ–æ— æ•ˆã€‚ä¿å­˜åˆ° NULL.txtã€‚")
                    save_null_data(api_url, content)
                    return []

                # Base64 ä¿®æ­£
                padding = len(content) % 4
                if padding:
                    content += '=' * (4 - padding)
                try:
                    decoded = base64.b64decode(content).decode('utf-8', errors='ignore')
                except Exception as e:
                    print(f"[é”™è¯¯] Base64 è§£ç å¤±è´¥ï¼š{api_url} -> {e}")
                    save_null_data(api_url, content)
                    return []

                lines = [line.strip() for line in decoded.splitlines() if line.strip()]
                return lines
    except Exception as e:
        print(f"Error processing {api_url}: {e}")
        save_null_data(api_url, str(e))
    return []

async def process_subscriptions(session, subscriptions):
    proxy_lines = []
    tasks = []
    for sub_url in subscriptions:
        for domain in CHECK_URL_LIST:
            tasks.append(convert_sub(session, sub_url, domain))
    results = await asyncio.gather(*tasks, return_exceptions=True)
    for result in results:
        if not isinstance(result, Exception) and result:
            proxy_lines.extend(result)
    return proxy_lines

# ========== ä¸»æµç¨‹ ==========

async def main():
    global subscriptions
    global config

    async with aiohttp.ClientSession() as session:
        # å¤„ç† Telegram é¢‘é“
        new_links = await process_tgchannels(session, tgchannels)

        # æ›´æ–° subscriptions
        subscriptions.extend(new_links)
        subscriptions = list(set(subscriptions))
        config['subscriptions'] = subscriptions

        with open('pool.yaml', 'w') as f:
            yaml.dump(config, f)

        # åˆ›å»º pool æ–‡ä»¶å¤¹
        os.makedirs('pool', exist_ok=True)

        # å¤„ç†è®¢é˜…
        proxy_lines = await process_subscriptions(session, subscriptions)

        # åˆ†ç±»å†™å…¥æ–‡ä»¶
        proxy_dict = {}
        for line in proxy_lines:
            try:
                parsed = urlparse(line)
                if parsed.scheme:
                    proxy_type = parsed.scheme
                    proxy_dict.setdefault(proxy_type, []).append(line)
            except Exception as e:
                print(f"[è­¦å‘Š] æ— æ•ˆé“¾æ¥è¢«è·³è¿‡: {line[:80]}... åŸå› : {e}")
                save_null_data("Invalid proxy line", f"{line}\nåŸå› : {e}")

        # å†™å…¥æ–‡ä»¶ï¼ˆå»é‡ + æ’åºï¼‰
        for proxy_type, new_lines in proxy_dict.items():
            file_path = os.path.join('pool', f"{proxy_type}.txt")
            existing_lines = set()
            if os.path.exists(file_path):
                with open(file_path, 'r', encoding='utf-8') as f:
                    existing_lines = set(line.strip() for line in f if line.strip())
            new_set = set(line.strip() for line in new_lines if line.strip())
            all_lines = sorted(existing_lines.union(new_set))
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(all_lines) + '\n')

# è¿è¡Œä¸»å‡½æ•°
if __name__ == "__main__":
    asyncio.run(main())
