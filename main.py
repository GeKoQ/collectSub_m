import asyncio
import aiohttp
import re
import yaml
import os
import base64
from urllib.parse import quote, unquote
from tqdm import tqdm
from loguru import logger
import json  # æ–°å¢ï¼šç”¨äºå¯èƒ½çš„ JSON è§£æ

# å…¨å±€é…ç½®ï¼ˆä¿æŒåŸæ ·ï¼‰
RE_URL = r"https?://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]"
CHECK_NODE_URL_STR = "https://{}/sub?target={}&url={}&insert=false&config=config%2FACL4SSR.ini"
CHECK_URL_LIST = ['api.dler.io', 'sub.xeton.dev', 'sub.id9.cc', 'sub.maoxiongnet.com']

# ä¿®æ”¹ï¼šUser-Agent åˆ—è¡¨ï¼Œé¡ºåºæµ‹è¯•
USER_AGENTS = [
    'v2rayNG/1.10.23',
    'NekoBox/Android/1.4.0(Prefer ClashMeta Format)',
    'sing-box',
    'ClashForWindows',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36 Edg/122.0.0.0'
]

# -------------------------------
# é…ç½®æ–‡ä»¶æ“ä½œï¼ˆä¿æŒåŸæ ·ï¼‰
# -------------------------------
def load_yaml_config(path_yaml):
    """è¯»å– YAML é…ç½®æ–‡ä»¶ï¼Œå¦‚æ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›é»˜è®¤ç»“æ„"""
    if os.path.exists(path_yaml):
        with open(path_yaml, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    else:
        config = {
            "æœºåœºè®¢é˜…": [],
            "clashè®¢é˜…": [],
            "v2è®¢é˜…": [],
            "å¼€å¿ƒç©è€": [],
            "tgchannel": []
        }
    return config

def save_yaml_config(config, path_yaml):
    """ä¿å­˜é…ç½®åˆ° YAML æ–‡ä»¶"""
    with open(path_yaml, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, allow_unicode=True)

def get_config_channels(config_file='config.yaml'):
    """
    ä»é…ç½®æ–‡ä»¶ä¸­è·å– Telegram é¢‘é“é“¾æ¥ï¼Œ
    å°†ç±»ä¼¼ https://t.me/univstar è½¬æ¢ä¸º https://t.me/s/univstar æ ¼å¼
    """
    config = load_yaml_config(config_file)
    tgchannels = config.get('tgchannel', [])
    new_list = []
    for url in tgchannels:
        parts = url.strip().split('/')
        if parts:
            channel_id = parts[-1]
            new_list.append(f'https://t.me/s/{channel_id}')
    return new_list

# -------------------------------
# å¼‚æ­¥ HTTP è¯·æ±‚è¾…åŠ©å‡½æ•°ï¼ˆä¿®æ”¹ï¼šé¡ºåºå°è¯•å¤šä¸ª User-Agentï¼‰
# -------------------------------
async def fetch_content(url, session, method='GET', headers=None, timeout=15):
    """è·å–æŒ‡å®š URL çš„æ–‡æœ¬å†…å®¹ï¼Œé¡ºåºå°è¯• User-Agent"""
    base_headers = {
        'Accept': '*/*',
        'Accept-Encoding': 'gzip, deflate'
    }
    if headers:
        base_headers.update(headers)
    
    for ua_index, user_agent in enumerate(USER_AGENTS):
        request_headers = base_headers.copy()
        request_headers['User-Agent'] = user_agent
        
        try:
            async with session.request(method, url, headers=request_headers, timeout=timeout) as response:
                if response.status == 200:
                    text = await response.text()
                    logger.debug(f"è¯·æ±‚ {url} æˆåŠŸï¼Œä½¿ç”¨ UA: {user_agent} (ç¬¬ {ua_index + 1} ä¸ª)")
                    return text
                else:
                    logger.warning(f"URL {url} è¿”å›çŠ¶æ€ {response.status}ï¼Œä½¿ç”¨ UA: {user_agent} (ç¬¬ {ua_index + 1} ä¸ª)")
        except asyncio.TimeoutError:
            logger.warning(f"è¯·æ±‚ {url} è¶…æ—¶ï¼Œä½¿ç”¨ UA: {user_agent} (ç¬¬ {ua_index + 1} ä¸ª)")
        except asyncio.CancelledError:
            logger.warning(f"è¯·æ±‚ {url} è¢«å–æ¶ˆï¼Œä½¿ç”¨ UA: {user_agent} (ç¬¬ {ua_index + 1} ä¸ª)")
            return None
        except Exception as e:
            logger.error(f"è¯·æ±‚ {url} å¼‚å¸¸: {e}ï¼Œä½¿ç”¨ UA: {user_agent} (ç¬¬ {ua_index + 1} ä¸ª)")
        
        # é™¤äº†æœ€åä¸€ä¸ªï¼Œç¨ä½œå»¶è¿Ÿå†è¯•ä¸‹ä¸€ä¸ª
        if ua_index < len(USER_AGENTS) - 1:
            await asyncio.sleep(0.5)
    
    logger.error(f"æ‰€æœ‰ User-Agent å°è¯•å¤±è´¥: {url}")
    return None

# -------------------------------
# æ–°å¢ï¼šè®¢é˜…è§£æå‡½æ•°ï¼ˆä¿æŒåŸæ ·ï¼‰
# -------------------------------
async def parse_subscription_content(content, sub_type):
    """
    è§£æè®¢é˜…å†…å®¹ï¼Œæ ¹æ®ç±»å‹æå–å¯å¯¼å…¥çš„èŠ‚ç‚¹é“¾æ¥ï¼ˆss://, vmess:// ç­‰ï¼‰
    è¿”å›å­—å…¸ï¼š{protocol: [links]}
    æ”¯æŒç±»å‹ï¼š'clash', 'v2', 'loon', 'sub' (æœºåœºï¼Œé€šå¸¸ base64 V2)
    """
    protocols = {
        'ss': [],
        'vmess': [],
        'trojan': [],
        'vless': [],
        'ssr': [],
        'other': []  # å…¶ä»–å¦‚ hysteria ç­‰
    }

    if not content or len(content.strip()) < 10:
        return protocols

    try:
        if sub_type == 'clash':
            # Clash YAML è§£æ
            config = yaml.safe_load(content)
            if 'proxies' in config:
                for proxy in config['proxies']:
                    p_type = proxy.get('type', '').lower()
                    name = proxy.get('name', 'Unnamed')
                    # æ ¹æ®ç±»å‹ç”Ÿæˆ share link
                    if p_type == 'ss':
                        server = proxy['server']
                        port = proxy['port']
                        method = proxy['cipher']
                        password = proxy['password']
                        link = f"ss://{quote(base64.b64encode(f'{method}:{password}@{server}:{port}'.encode()).decode())}#{quote(name)}"
                        protocols['ss'].append(link)
                    elif p_type == 'vmess':
                        # VMess JSON to link
                        v_obj = {
                            "v": "2",
                            "ps": name,
                            "add": proxy['server'],
                            "port": str(proxy['port']),
                            "id": proxy['uuid'],
                            "aid": str(proxy.get('alterId', 0)),
                            "net": proxy.get('network', 'tcp'),
                            "type": "none",
                            "host": proxy.get('ws-headers', {}).get('Host', ''),
                            "path": proxy.get('ws-path', ''),
                            "tls": proxy.get('tls', '')
                        }
                        json_str = json.dumps(v_obj)
                        link = f"vmess://{quote(base64.b64encode(json_str.encode()).decode())}"
                        protocols['vmess'].append(link)
                    elif p_type == 'trojan':
                        server = proxy['server']
                        port = proxy['port']
                        password = proxy['password']
                        link = f"trojan://{quote(password)}@{server}:{port}#{quote(name)}"
                        protocols['trojan'].append(link)
                    elif p_type == 'vless':
                        server = proxy['server']
                        port = proxy['port']
                        uuid = proxy['uuid']
                        link = f"vless://{quote(uuid)}@{server}:{port}#{quote(name)}"
                        protocols['vless'].append(link)
                    else:
                        protocols['other'].append(f"clash://{p_type}:{name}")  # ç®€åŒ–å…¶ä»–ç±»å‹

        elif sub_type in ['v2', 'sub']:  # V2 å’Œæœºåœºé€šå¸¸ base64 ç¼–ç çš„é“¾æ¥åˆ—è¡¨
            # å°è¯• base64 è§£ç 
            try:
                decoded = base64.b64decode(content.strip()).decode('utf-8', errors='ignore')
                lines = [line.strip() for line in decoded.split('\n') if line.strip()]
            except:
                lines = [line.strip() for line in content.split('\n') if line.strip()]  # å¦‚æœä¸æ˜¯ base64ï¼Œå‡è®¾åŸå§‹

            for line in lines:
                if line.startswith('ss://'):
                    protocols['ss'].append(line)
                elif line.startswith('vmess://'):
                    protocols['vmess'].append(line)
                elif line.startswith('trojan://'):
                    protocols['trojan'].append(line)
                elif line.startswith('vless://'):
                    protocols['vless'].append(line)
                elif line.startswith('ssr://'):
                    protocols['ssr'].append(line)
                else:
                    protocols['other'].append(line)

        elif sub_type == 'loon':
            # Loon æ ¼å¼ï¼šæ¯è¡Œ proxy = url, name æˆ–ç›´æ¥ URI
            lines = [line.strip() for line in content.split('\n') if line.strip() and '=' in line]
            for line in lines:
                if line.startswith('[Proxy]'):
                    continue
                parts = line.split('=', 1)
                if len(parts) == 2:
                    url_part = parts[1].strip().strip('"')
                    if url_part.startswith('ss://'):
                        protocols['ss'].append(url_part)
                    elif url_part.startswith('vmess://'):
                        protocols['vmess'].append(url_part)
                    elif url_part.startswith('trojan://'):
                        protocols['trojan'].append(url_part)
                    elif url_part.startswith('vless://'):
                        protocols['vless'].append(url_part)
                    else:
                        protocols['other'].append(url_part)

    except Exception as e:
        logger.error(f"è§£æ {sub_type} è®¢é˜…å†…å®¹å¼‚å¸¸: {e}")
        protocols['other'].append(content[:200])  # ä¿ç•™åŸå§‹ç‰‡æ®µç”¨äºè°ƒè¯•

    # è¿‡æ»¤ç©ºåˆ—è¡¨
    protocols = {k: v for k, v in protocols.items() if v}
    return protocols

async def download_and_process_all_txt(all_txt_path, sub_dir='sub'):
    """
    ä» all.txt ä¸‹è½½è®¢é˜…ä¿¡æ¯åˆ° sub/ æ–‡ä»¶å¤¹ï¼ŒæŒ‰ä»£ç†ç±»å‹åˆ†ç±»ä¿å­˜é“¾æ¥
    """
    if not os.path.exists(all_txt_path):
        logger.error(f"all.txt ä¸å­˜åœ¨: {all_txt_path}")
        return

    # åˆ›å»º sub/ æ–‡ä»¶å¤¹
    os.makedirs(sub_dir, exist_ok=True)

    # è¯»å– all.txt å¹¶åˆ†å‰²éƒ¨åˆ†
    with open(all_txt_path, 'r', encoding='utf-8') as f:
        content = f.read()

    sections = re.split(r'--\s*(\w+)\s*--', content)  # åˆ†å‰² -- Section --
    urls_by_type = {}  # {type: [urls]}

    for i in range(1, len(sections), 2):  # è·³è¿‡ç©ºéƒ¨åˆ†
        section_name = sections[i].strip().lower()
        section_content = sections[i+1].strip() if i+1 < len(sections) else ''
        urls = re.findall(RE_URL, section_content)
        if urls:
            # æ˜ å°„ section åˆ° sub_type
            if 'sub store' in section_name:
                sub_type = 'sub'  # æœºåœºï¼Œé€šå¸¸ V2 base64
            elif 'loon' in section_name:
                sub_type = 'loon'
            elif 'clash' in section_name:
                sub_type = 'clash'
            elif 'v2' in section_name:
                sub_type = 'v2'
            else:
                sub_type = 'other'
            urls_by_type[sub_type] = list(set(urls))  # å»é‡

    logger.info(f"ä» all.txt æå–è®¢é˜…ç±»å‹: {urls_by_type.keys()}")

    # å¹¶å‘ä¸‹è½½å’Œè§£æ - ä½¿ç”¨å•ä¸ª session
    connector = aiohttp.TCPConnector(limit=50)
    timeout = aiohttp.ClientTimeout(total=60, connect=15)  # å¢åŠ è¶…æ—¶æ—¶é—´
    semaphore = asyncio.Semaphore(10)  # é™ä½å¹¶å‘æ•°ï¼Œé¿å… Actions ç½‘ç»œå‹åŠ›

    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        async def process_single_url(url, sub_type):
            async with semaphore:
                try:
                    logger.debug(f"å¼€å§‹å¤„ç† URL: {url}")
                    content = await fetch_content(url, session, timeout=aiohttp.ClientTimeout(total=30))
                    if content:
                        return await parse_subscription_content(content, sub_type)
                    else:
                        logger.debug(f"URL {url} æ— å†…å®¹")
                    return {}
                except asyncio.CancelledError:
                    logger.warning(f"Task for {url} was cancelled")
                    return {}
                except Exception as e:
                    logger.error(f"Error processing {url}: {e}")
                    return {}

        all_protocols = {p: [] for p in ['ss', 'vmess', 'trojan', 'vless', 'ssr', 'other']}

        for sub_type, urls in urls_by_type.items():
            logger.info(f"å¤„ç† {sub_type} ç±»å‹: {len(urls)} ä¸ª URL")
            tasks = [process_single_url(url, sub_type) for url in urls]
            completed = 0
            for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=f"è§£æ{sub_type}"):
                try:
                    protocols = await coro
                    for proto, links in protocols.items():
                        all_protocols[proto].extend(links)
                    completed += 1
                except asyncio.CancelledError:
                    logger.warning(f"as_completed for {sub_type} was cancelled")
                except Exception as e:
                    logger.error(f"Error in as_completed for {sub_type}: {e}")
                    completed += 1
            logger.info(f"{sub_type} å¤„ç†å®Œæˆ: {completed}/{len(urls)}")

        # ä¿å­˜åˆ°æ–‡ä»¶
        for proto, links in all_protocols.items():
            unique_links = sorted(set(links))
            if unique_links:
                file_path = os.path.join(sub_dir, f"{proto}_links.txt")
                # ä¿®æ”¹ï¼šè¿½åŠ æ¨¡å¼ ('a')ï¼Œå¹¶å¤„ç†é¦–æ¬¡å†™å…¥
                with open(file_path, 'a', encoding='utf-8') as f:
                   if os.path.getsize(file_path) == 0:  # å¦‚æœæ–‡ä»¶ä¸ºç©ºï¼Œæ·»åŠ æ ‡é¢˜
                       f.write(f"# {proto.upper()} Links (Updated: {asyncio.get_event_loop().time()})\n\n")
                   f.write("\n".join(unique_links) + "\n")  # è¿½åŠ é“¾æ¥ï¼Œæ¯è¡Œä¸€ä¸ª
                logger.info(f"è¿½åŠ ä¿å­˜ {len(unique_links)} ä¸ª {proto} é“¾æ¥åˆ° {file_path}")

# -------------------------------
# é¢‘é“æŠ“å–åŠè®¢é˜…æ£€æŸ¥ï¼ˆä¿®æ”¹ï¼šä½¿ç”¨é¡ºåº User-Agentï¼‰
# -------------------------------
async def get_channel_urls(channel_url, session):
    """ä» Telegram é¢‘é“é¡µé¢æŠ“å–æ‰€æœ‰è®¢é˜…é“¾æ¥ï¼Œå¹¶è¿‡æ»¤æ— å…³é“¾æ¥"""
    content = await fetch_content(channel_url, session)
    if content:
        # æå–æ‰€æœ‰ URLï¼Œå¹¶æ’é™¤åŒ…å«â€œ//t.me/â€æˆ–â€œcdn-telegram.orgâ€çš„é“¾æ¥
        all_urls = re.findall(RE_URL, content)
        filtered = [u for u in all_urls if "//t.me/" not in u and "cdn-telegram.org" not in u]
        logger.info(f"ä» {channel_url} æå– {len(filtered)} ä¸ªé“¾æ¥")
        return filtered
    else:
        logger.warning(f"æ— æ³•è·å– {channel_url} çš„å†…å®¹")
        return []

async def sub_check(url, session):
    """
    æ”¹è¿›çš„è®¢é˜…æ£€æŸ¥å‡½æ•°ï¼š
      - åˆ¤æ–­å“åº”å¤´ä¸­çš„ subscription-userinfo ç”¨äºæœºåœºè®¢é˜…
      - åˆ¤æ–­å†…å®¹ä¸­æ˜¯å¦åŒ…å« 'proxies:' åˆ¤å®š clash è®¢é˜…
      - å°è¯• base64 è§£ç åˆ¤æ–­ v2 è®¢é˜…ï¼ˆè¯†åˆ« ss://ã€ssr://ã€vmess://ã€trojan://ã€vless://ï¼‰
      - å¢åŠ é‡è¯•æœºåˆ¶å’Œæ›´å¥½çš„é”™è¯¯å¤„ç†
    è¿”å›ä¸€ä¸ªå­—å…¸ï¼š{"url": ..., "type": ..., "info": ...}
    """
    base_headers = {
        'Accept': '*/*',
        'Accept-Encoding': 'gzip, deflate'
    }

    # é‡è¯•æœºåˆ¶ï¼ˆåŒ…æ‹¬ UA å°è¯•ï¼‰
    for attempt in range(2):
        success = False
        for ua_index, user_agent in enumerate(USER_AGENTS):
            headers = base_headers.copy()
            headers['User-Agent'] = user_agent
            
            try:
                async with session.get(url, headers=headers, timeout=12) as response:
                    if response.status == 200:
                        text = await response.text()

                        # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–è¿‡çŸ­
                        if not text or len(text.strip()) < 10:
                            logger.debug(f"è®¢é˜… {url} å†…å®¹ä¸ºç©ºæˆ–è¿‡çŸ­ï¼Œä½¿ç”¨ UA: {user_agent}")
                            continue  # å°è¯•ä¸‹ä¸€ä¸ª UA

                        result = {"url": url, "type": None, "info": None}

                        # åˆ¤æ–­æœºåœºè®¢é˜…ï¼ˆæ£€æŸ¥æµé‡ä¿¡æ¯ï¼‰
                        sub_info = response.headers.get('subscription-userinfo')
                        if sub_info:
                            nums = re.findall(r'\d+', sub_info)
                            if len(nums) >= 3:
                                upload, download, total = map(int, nums[:3])
                                if total > 0:  # ç¡®ä¿æ€»æµé‡å¤§äº0
                                    unused = (total - upload - download) / (1024 ** 3)
                                    if unused > 0:
                                        result["type"] = "æœºåœºè®¢é˜…"
                                        result["info"] = f"å¯ç”¨æµé‡: {round(unused, 2)} GB"
                                        logger.debug(f"è®¢é˜… {url} æˆåŠŸ (æœºåœº)ï¼Œä½¿ç”¨ UA: {user_agent}")
                                        return result

                        # åˆ¤æ–­ clash è®¢é˜… - æ›´ä¸¥æ ¼çš„æ£€æŸ¥
                        if "proxies:" in text and ("name:" in text or "server:" in text):
                            proxy_count = text.count("- name:")
                            if proxy_count > 0:
                                result["type"] = "clashè®¢é˜…"
                                result["info"] = f"åŒ…å« {proxy_count} ä¸ªèŠ‚ç‚¹"
                                logger.debug(f"è®¢é˜… {url} æˆåŠŸ (clash)ï¼Œä½¿ç”¨ UA: {user_agent}")
                                return result

                        # åˆ¤æ–­ v2 è®¢é˜…ï¼Œé€šè¿‡ base64 è§£ç æ£€æµ‹
                        try:
                            # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯base64ç¼–ç ï¼ˆæ›´å®½æ¾çš„æ£€æŸ¥ï¼‰
                            text_clean = text.strip().replace('\n', '').replace('\r', '')
                            if len(text_clean) > 20:
                                try:
                                    # å°è¯•è§£ç 
                                    decoded = base64.b64decode(text_clean).decode('utf-8', errors='ignore')
                                    protocols = ['ss://', 'ssr://', 'vmess://', 'trojan://', 'vless://']
                                    found_protocols = [proto for proto in protocols if proto in decoded]

                                    if found_protocols:
                                        node_count = sum(decoded.count(proto) for proto in found_protocols)
                                        if node_count > 0:
                                            result["type"] = "v2è®¢é˜…"
                                            result["info"] = f"åŒ…å« {node_count} ä¸ªèŠ‚ç‚¹ (base64)"
                                            logger.debug(f"è®¢é˜… {url} æˆåŠŸ (v2 base64)ï¼Œä½¿ç”¨ UA: {user_agent}")
                                            return result
                                    else:
                                        # æ£€æŸ¥è§£ç åæ˜¯å¦åŒ…å«é…ç½®å…³é”®å­—
                                        config_keywords = ['server', 'port', 'password', 'method', 'host', 'path']
                                        if any(keyword in decoded.lower() for keyword in config_keywords):
                                            lines = [line.strip() for line in decoded.split('\n') if line.strip()]
                                            if len(lines) > 0:
                                                result["type"] = "v2è®¢é˜…"
                                                result["info"] = f"åŒ…å« {len(lines)} è¡Œé…ç½® (base64)"
                                                logger.debug(f"è®¢é˜… {url} æˆåŠŸ (v2 config)ï¼Œä½¿ç”¨ UA: {user_agent}")
                                                return result
                                except Exception:
                                    # base64è§£ç å¤±è´¥ï¼Œç»§ç»­å…¶ä»–æ£€æŸ¥
                                    pass
                        except Exception as e:
                            logger.debug(f"è®¢é˜… {url} base64æ£€æµ‹å¼‚å¸¸: {e}ï¼Œä½¿ç”¨ UA: {user_agent}")
                            pass

                        # æ£€æŸ¥æ˜¯å¦æ˜¯åŸå§‹æ ¼å¼çš„v2è®¢é˜…
                        protocols = ['ss://', 'ssr://', 'vmess://', 'trojan://', 'vless://']
                        found_protocols = [proto for proto in protocols if proto in text]
                        if found_protocols:
                            node_count = sum(text.count(proto) for proto in found_protocols)
                            if node_count > 0:
                                result["type"] = "v2è®¢é˜…"
                                result["info"] = f"åŒ…å« {node_count} ä¸ªèŠ‚ç‚¹ (åŸå§‹)"
                                logger.debug(f"è®¢é˜… {url} æˆåŠŸ (v2 åŸå§‹)ï¼Œä½¿ç”¨ UA: {user_agent}")
                                return result

                        # å¦‚æœå†…å®¹çœ‹èµ·æ¥åƒé…ç½®ä½†ä¸åŒ¹é…å·²çŸ¥æ ¼å¼ï¼Œè®°å½•è°ƒè¯•ä¿¡æ¯
                        if len(text) > 100:
                            # æ˜¾ç¤ºå†…å®¹çš„å‰100ä¸ªå­—ç¬¦ç”¨äºè°ƒè¯•
                            preview = text[:100].replace('\n', '\\n').replace('\r', '\\r')
                            logger.info(f"âš ï¸  è®¢é˜… {url} å†…å®¹ä¸åŒ¹é…å·²çŸ¥æ ¼å¼ï¼Œä½¿ç”¨ UA: {user_agent}")
                            logger.info(f"   é•¿åº¦: {len(text)} å­—ç¬¦")
                            logger.info(f"   é¢„è§ˆ: {preview}...")

                            # æ£€æŸ¥æ˜¯å¦å¯èƒ½æ˜¯å…¶ä»–æ ¼å¼
                            if 'http' in text.lower() or 'server' in text.lower():
                                logger.info(f"   å¯èƒ½åŒ…å«æœåŠ¡å™¨é…ç½®ï¼Œä½†æ ¼å¼æœªè¯†åˆ«")

                        success = True  # å³ä½¿ä¸åŒ¹é…ç±»å‹ï¼Œä¹Ÿè§†ä¸ºæˆåŠŸï¼ˆé¿å…æ— é™é‡è¯•ï¼‰
                        return None

                    elif response.status in [403, 404, 410, 500]:
                        # è¿™äº›çŠ¶æ€ç é€šå¸¸è¡¨ç¤ºæ°¸ä¹…å¤±è´¥
                        logger.debug(f"è®¢é˜…æ£€æŸ¥ {url} è¿”å›çŠ¶æ€ {response.status}ï¼Œä½¿ç”¨ UA: {user_agent}")
                        continue  # å°è¯•ä¸‹ä¸€ä¸ª UA
                    else:
                        logger.warning(f"è®¢é˜…æ£€æŸ¥ {url} è¿”å›çŠ¶æ€ {response.status}ï¼Œä½¿ç”¨ UA: {user_agent}")
                        if ua_index < len(USER_AGENTS) - 1:
                            await asyncio.sleep(0.5)  # å»¶è¿Ÿå†è¯•ä¸‹ä¸€ä¸ª UA
                            continue
                        else:
                            return None

            except asyncio.TimeoutError:
                logger.debug(f"è®¢é˜…æ£€æŸ¥ {url} è¶…æ—¶ï¼Œä½¿ç”¨ UA: {user_agent}ï¼Œå°è¯• {attempt + 1}/2")
                continue  # å°è¯•ä¸‹ä¸€ä¸ª UA
            except Exception as e:
                logger.debug(f"è®¢é˜…æ£€æŸ¥ {url} å¼‚å¸¸: {e}ï¼Œä½¿ç”¨ UA: {user_agent}ï¼Œå°è¯• {attempt + 1}/2")
                continue  # å°è¯•ä¸‹ä¸€ä¸ª UA

        if success:
            break  # å¦‚æœæˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯

        if attempt == 0:  # ç¬¬ä¸€æ¬¡å¤±è´¥ï¼Œé‡è¯•æ•´ä¸ªè¿‡ç¨‹
            await asyncio.sleep(1)

    return None

# -------------------------------
# èŠ‚ç‚¹æœ‰æ•ˆæ€§æ£€æµ‹ï¼ˆä¿®æ”¹ï¼šæ·»åŠ å¼‚å¸¸å¤„ç†ï¼Œå¹¶é¡ºåºå°è¯• User-Agentï¼‰
# -------------------------------
async def url_check_valid(url, target, session):
    """
    æ”¹è¿›çš„èŠ‚ç‚¹æœ‰æ•ˆæ€§æ£€æµ‹ï¼š
    é€šè¿‡éå†å¤šä¸ªæ£€æµ‹å…¥å£æ£€æŸ¥è®¢é˜…èŠ‚ç‚¹æœ‰æ•ˆæ€§ï¼Œ
    ä¸ä»…æ£€æŸ¥çŠ¶æ€ç ï¼Œè¿˜éªŒè¯è¿”å›å†…å®¹çš„æœ‰æ•ˆæ€§ã€‚
    """
    encoded_url = quote(url, safe='')

    for check_base in CHECK_URL_LIST:
        success = False
        for ua_index, user_agent in enumerate(USER_AGENTS):
            headers = {'User-Agent': user_agent}
            check_url = CHECK_NODE_URL_STR.format(check_base, target, encoded_url)
            try:
                async with session.get(check_url, headers=headers, timeout=20) as resp:
                    if resp.status == 200:
                        content = await resp.text()

                        # æ£€æŸ¥è¿”å›å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                        if not content or len(content.strip()) < 50:
                            logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} è¿”å›å†…å®¹è¿‡çŸ­ï¼Œä½¿ç”¨ UA: {user_agent}")
                            continue  # å°è¯•ä¸‹ä¸€ä¸ª UA

                        # æ ¹æ®ç›®æ ‡ç±»å‹éªŒè¯å†…å®¹
                        if target == "clash":
                            if "proxies:" in content and ("name:" in content or "server:" in content):
                                proxy_count = content.count("- name:")
                                if proxy_count > 0:
                                    logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} æˆåŠŸï¼ŒåŒ…å« {proxy_count} ä¸ªèŠ‚ç‚¹ï¼Œä½¿ç”¨ UA: {user_agent}")
                                    return url
                        elif target == "loon":
                            # Loonæ ¼å¼é€šå¸¸åŒ…å«[Proxy]æ®µè½
                            if "[Proxy]" in content or "=" in content:
                                logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} æˆåŠŸ (Loonæ ¼å¼)ï¼Œä½¿ç”¨ UA: {user_agent}")
                                return url
                        elif target == "v2ray":
                            # V2Rayæ ¼å¼å¯èƒ½æ˜¯JSONæˆ–å…¶ä»–æ ¼å¼
                            if len(content.strip()) > 100:  # åŸºæœ¬é•¿åº¦æ£€æŸ¥
                                logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} æˆåŠŸ (V2Rayæ ¼å¼)ï¼Œä½¿ç”¨ UA: {user_agent}")
                                return url
                        else:
                            # å…¶ä»–æ ¼å¼ï¼ŒåŸºæœ¬é•¿åº¦æ£€æŸ¥
                            if len(content.strip()) > 100:
                                logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} æˆåŠŸï¼Œä½¿ç”¨ UA: {user_agent}")
                                return url

                        logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} å†…å®¹æ ¼å¼ä¸åŒ¹é…ï¼Œä½¿ç”¨ UA: {user_agent}")
                        success = True  # è§†ä¸ºæˆåŠŸï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæ£€æµ‹ç‚¹
                        break  # è·³å‡º UA å¾ªç¯

                    else:
                        logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} è¿”å›çŠ¶æ€ {resp.status}ï¼Œä½¿ç”¨ UA: {user_agent}")
                        if ua_index < len(USER_AGENTS) - 1:
                            await asyncio.sleep(0.5)
                            continue
                        else:
                            break  # UA ç”¨å®Œï¼Œå°è¯•ä¸‹ä¸€ä¸ª check_base

            except asyncio.TimeoutError:
                logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} è¶…æ—¶ï¼Œä½¿ç”¨ UA: {user_agent}")
                continue  # å°è¯•ä¸‹ä¸€ä¸ª UA
            except asyncio.CancelledError:
                logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} è¢«å–æ¶ˆï¼Œä½¿ç”¨ UA: {user_agent}")
                return None
            except Exception as e:
                logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨ {check_base} å¼‚å¸¸: {e}ï¼Œä½¿ç”¨ UA: {user_agent}")
                continue  # å°è¯•ä¸‹ä¸€ä¸ª UA

        if success:
            break  # å¦‚æœæˆåŠŸï¼Œè·³å‡º check_base å¾ªç¯

    logger.debug(f"èŠ‚ç‚¹æ£€æµ‹ {url} åœ¨æ‰€æœ‰æ£€æµ‹ç‚¹éƒ½å¤±è´¥")
    return None

# -------------------------------
# ä¸»æµç¨‹ï¼šæ›´æ–°è®¢é˜…ä¸åˆå¹¶ï¼ˆä¿æŒåŸæ ·ï¼Œä½† main ä¸­æ·»åŠ æ–°æ­¥éª¤ï¼‰
# -------------------------------
async def update_today_sub(session):
    """
    ä» Telegram é¢‘é“è·å–æœ€æ–°è®¢é˜…é“¾æ¥ï¼Œ
    è¿”å›ä¸€ä¸ªå»é‡åçš„ URL åˆ—è¡¨
    """
    tg_channels = get_config_channels('config.yaml')
    all_urls = []
    for channel in tg_channels:
        urls = await get_channel_urls(channel, session)
        all_urls.extend(urls)
    return list(set(all_urls))

async def check_subscriptions(urls):
    """
    å¼‚æ­¥æ£€æŸ¥æ‰€æœ‰è®¢é˜…é“¾æ¥çš„æœ‰æ•ˆæ€§ï¼Œ
    è¿”å›æ£€æŸ¥ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœä¸ºå­—å…¸ {url, type, info}
    """
    if not urls:
        return []

    results = []
    # åˆ›å»ºè¿æ¥å™¨ï¼Œé™åˆ¶å¹¶å‘è¿æ¥æ•°
    connector = aiohttp.TCPConnector(
        limit=100,
        limit_per_host=20,
        ttl_dns_cache=300,
        use_dns_cache=True,
    )

    timeout = aiohttp.ClientTimeout(total=30, connect=10)

    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
        # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(50)

        async def check_single(url):
            async with semaphore:
                return await sub_check(url, session)

        tasks = [check_single(url) for url in urls]
        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="è®¢é˜…ç­›é€‰"):
            try:
                res = await coro
                if res:
                    results.append(res)
            except Exception as e:
                logger.error(f"Error in check_subscriptions: {e}")

    return results

async def check_nodes(urls, target, session):
    """
    å¼‚æ­¥æ£€æŸ¥æ¯ä¸ªè®¢é˜…èŠ‚ç‚¹çš„æœ‰æ•ˆæ€§ï¼Œ
    è¿”å›æ£€æµ‹æœ‰æ•ˆçš„èŠ‚ç‚¹ URL åˆ—è¡¨
    """
    if not urls:
        return []

    valid_urls = []
    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
    semaphore = asyncio.Semaphore(20)  # èŠ‚ç‚¹æ£€æµ‹å¹¶å‘æ•°è¾ƒä½ï¼Œé¿å…è¢«å°

    async def check_single_node(url):
        async with semaphore:
            return await url_check_valid(url, target, session)

    tasks = [check_single_node(url) for url in urls]
    for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc=f"æ£€æµ‹{target}èŠ‚ç‚¹"):
        try:
            res = await coro
            if res:
                valid_urls.append(res)
        except Exception as e:
            logger.error(f"Error in check_nodes: {e}")

    return valid_urls

def write_url_list(url_list, file_path):
    """å°† URL åˆ—è¡¨å†™å…¥æ–‡æœ¬æ–‡ä»¶"""
    with open(file_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(url_list))
    logger.info(f"å·²ä¿å­˜ {len(url_list)} ä¸ªé“¾æ¥åˆ° {file_path}")

def merge_files_to_all_txt(sub_store_file, loon_file, clash_file, v2_file, all_file):
    """å°†å¤šä¸ªé…ç½®æ–‡ä»¶åˆå¹¶åˆ° all.txt"""
    merged_content = []
    
    # æ·»åŠ  Sub Store å†…å®¹
    if os.path.exists(sub_store_file):
        with open(sub_store_file, 'r', encoding='utf-8') as f:
            sub_content = f.read()
        merged_content.append("-- Sub Store --")
        merged_content.append(sub_content)
        merged_content.append("")
    
    # æ·»åŠ  Loon å†…å®¹
    if os.path.exists(loon_file):
        with open(loon_file, 'r', encoding='utf-8') as f:
            loon_content = f.read().strip()
        if loon_content:
            merged_content.append("-- Loon --")
            merged_content.append(loon_content)
            merged_content.append("")
    
    # æ·»åŠ  Clash å†…å®¹
    if os.path.exists(clash_file):
        with open(clash_file, 'r', encoding='utf-8') as f:
            clash_content = f.read().strip()
        if clash_content:
            merged_content.append("-- Clash --")
            merged_content.append(clash_content)
            merged_content.append("")
    
    # æ·»åŠ  V2 å†…å®¹
    if os.path.exists(v2_file):
        with open(v2_file, 'r', encoding='utf-8') as f:
            v2_content = f.read().strip()
        if v2_content:
            merged_content.append("-- V2 --")
            merged_content.append(v2_content)
            merged_content.append("")
    
    # å†™å…¥ all.txt
    with open(all_file, 'w', encoding='utf-8') as f:
        f.write("\n".join(merged_content))
    logger.info(f"ğŸ“„ å·²åˆå¹¶ç”Ÿæˆ: {all_file}")

# -------------------------------
# ä¸»å‡½æ•°å…¥å£ï¼ˆä¿®æ”¹ï¼šæ·»åŠ ç¬¬å…«æ­¥å¤„ç† all.txtï¼‰
# -------------------------------
async def validate_existing_subscriptions(config, session):
    """éªŒè¯ç°æœ‰è®¢é˜…çš„æœ‰æ•ˆæ€§ï¼Œç§»é™¤å¤±æ•ˆè®¢é˜…"""
    logger.info("ğŸ” å¼€å§‹éªŒè¯ç°æœ‰è®¢é˜…çš„æœ‰æ•ˆæ€§...")

    all_existing_urls = []

    # æå–æ‰€æœ‰ç°æœ‰è®¢é˜…URL
    for category in ["æœºåœºè®¢é˜…", "clashè®¢é˜…", "v2è®¢é˜…"]:
        for item in config.get(category, []):
            if isinstance(item, str) and item.strip():
                all_existing_urls.append((item.strip(), category))

    # ä»å¼€å¿ƒç©è€ä¸­æå–URL
    for item in config.get("å¼€å¿ƒç©è€", []):
        if isinstance(item, str) and item.strip():
            url_match = re.search(r'https?://[^\s]+', item)
            if url_match:
                all_existing_urls.append((url_match.group(), "å¼€å¿ƒç©è€"))

    if not all_existing_urls:
        logger.info("ğŸ“ æ²¡æœ‰ç°æœ‰è®¢é˜…éœ€è¦éªŒè¯")
        return {"æœºåœºè®¢é˜…": [], "clashè®¢é˜…": [], "v2è®¢é˜…": [], "å¼€å¿ƒç©è€": []}

    logger.info(f"ğŸ“Š éœ€è¦éªŒè¯ {len(all_existing_urls)} ä¸ªç°æœ‰è®¢é˜…")

    # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘
    semaphore = asyncio.Semaphore(30)

    async def check_single_existing(url_info):
        url, category = url_info
        async with semaphore:
            result = await sub_check(url, session)
            return (url, category, result)

    valid_existing = {"æœºåœºè®¢é˜…": [], "clashè®¢é˜…": [], "v2è®¢é˜…": [], "å¼€å¿ƒç©è€": []}
    tasks = [check_single_existing(url_info) for url_info in all_existing_urls]

    for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="éªŒè¯ç°æœ‰è®¢é˜…"):
        try:
            url, category, result = await coro
            if result:
                if result["type"] == "æœºåœºè®¢é˜…":
                    valid_existing["æœºåœºè®¢é˜…"].append(url)
                    if result["info"]:
                        valid_existing["å¼€å¿ƒç©è€"].append(f'{result["info"]}\n{url}')
                elif result["type"] == "clashè®¢é˜…":
                    valid_existing["clashè®¢é˜…"].append(url)
                elif result["type"] == "v2è®¢é˜…":
                    valid_existing["v2è®¢é˜…"].append(url)
        except Exception as e:
            logger.error(f"Error in validate_existing: {e}")

    # ç»Ÿè®¡éªŒè¯ç»“æœ
    total_original = len(all_existing_urls)
    total_valid = sum(len(valid_existing[cat]) for cat in ["æœºåœºè®¢é˜…", "clashè®¢é˜…", "v2è®¢é˜…"])

    logger.info(f"âœ… ç°æœ‰è®¢é˜…éªŒè¯å®Œæˆ: {total_original} â†’ {total_valid} (æœ‰æ•ˆç‡: {total_valid/total_original*100:.1f}%)")

    return valid_existing

async def main():
    config_path = 'config.yaml'

    logger.info("ğŸš€ å¼€å§‹è®¢é˜…ç®¡ç†æµç¨‹...")
    logger.info("=" * 60)

    # åŠ è½½ç°æœ‰é…ç½®
    config = load_yaml_config(config_path)

    # ç»Ÿè®¡åŸå§‹æ•°æ®
    original_counts = {}
    for category in ["æœºåœºè®¢é˜…", "clashè®¢é˜…", "v2è®¢é˜…", "å¼€å¿ƒç©è€"]:
        original_counts[category] = len(config.get(category, []))

    logger.info("ğŸ“Š åŸå§‹é…ç½®ç»Ÿè®¡:")
    for category, count in original_counts.items():
        logger.info(f"   {category}: {count:,} ä¸ª")

    # åˆ›å»ºä¼˜åŒ–çš„ä¼šè¯
    connector = aiohttp.TCPConnector(
        limit=100,
        limit_per_host=20,
        ttl_dns_cache=300,
        use_dns_cache=True,
    )
    timeout = aiohttp.ClientTimeout(total=30, connect=10)

    async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:

        # ç¬¬ä¸€æ­¥ï¼šéªŒè¯ç°æœ‰è®¢é˜…
        logger.info("\nğŸ” ç¬¬ä¸€æ­¥ï¼šéªŒè¯ç°æœ‰è®¢é˜…")
        logger.info("-" * 40)
        valid_existing = await validate_existing_subscriptions(config, session)

        # ç¬¬äºŒæ­¥ï¼šè·å–æ–°çš„è®¢é˜…é“¾æ¥
        logger.info("\nğŸ“¡ ç¬¬äºŒæ­¥ï¼šè·å–æ–°çš„è®¢é˜…é“¾æ¥")
        logger.info("-" * 40)
        today_urls = await update_today_sub(session)
        logger.info(f"ğŸ“¥ ä» Telegram é¢‘é“è·å¾— {len(today_urls)} ä¸ªæ–°é“¾æ¥")

        # ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥æ–°è®¢é˜…çš„æœ‰æ•ˆæ€§
        logger.info("\nğŸ” ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥æ–°è®¢é˜…æœ‰æ•ˆæ€§")
        logger.info("-" * 40)
        new_results = await check_subscriptions(today_urls)

        # åˆ†ç±»æ–°è®¢é˜…
        new_subs = [res["url"] for res in new_results if res and res["type"] == "æœºåœºè®¢é˜…"]
        new_clash = [res["url"] for res in new_results if res and res["type"] == "clashè®¢é˜…"]
        new_v2 = [res["url"] for res in new_results if res and res["type"] == "v2è®¢é˜…"]
        new_play = [f'{res["info"]} {res["url"]}' for res in new_results 
                   if res and res["type"] == "æœºåœºè®¢é˜…" and res["info"]]

        logger.info(f"âœ… æ–°å¢æœ‰æ•ˆè®¢é˜…: æœºåœº{len(new_subs)}ä¸ª, clash{len(new_clash)}ä¸ª, v2{len(new_v2)}ä¸ª")

        # ç¬¬å››æ­¥ï¼šåˆå¹¶æœ‰æ•ˆè®¢é˜…
        logger.info("\nğŸ”„ ç¬¬å››æ­¥ï¼šåˆå¹¶æœ‰æ•ˆè®¢é˜…")
        logger.info("-" * 40)

        final_config = {
            "æœºåœºè®¢é˜…": sorted(list(set(valid_existing["æœºåœºè®¢é˜…"] + new_subs))),
            "clashè®¢é˜…": sorted(list(set(valid_existing["clashè®¢é˜…"] + new_clash))),
            "v2è®¢é˜…": sorted(list(set(valid_existing["v2è®¢é˜…"] + new_v2))),
            "å¼€å¿ƒç©è€": sorted(list(set(valid_existing["å¼€å¿ƒç©è€"] + new_play))),
            "tgchannel": config.get("tgchannel", [])  # ä¿ç•™é¢‘é“é…ç½®
        }

        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        logger.info("ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡å¯¹æ¯”:")
        total_original = sum(original_counts.values())
        total_final = sum(len(final_config[cat]) for cat in ["æœºåœºè®¢é˜…", "clashè®¢é˜…", "v2è®¢é˜…", "å¼€å¿ƒç©è€"])

        for category in ["æœºåœºè®¢é˜…", "clashè®¢é˜…", "v2è®¢é˜…", "å¼€å¿ƒç©è€"]:
            original = original_counts[category]
            final = len(final_config[category])
            change = final - original
            change_str = f"(+{change})" if change > 0 else f"({change})" if change < 0 else "(=)"
            logger.info(f"   {category}: {original:,} â†’ {final:,} {change_str}")

        logger.info(f"ğŸ“Š æ€»ä½“: {total_original:,} â†’ {total_final:,} "
                   f"(æ¸…ç†ç‡: {(total_original-total_final)/total_original*100:.1f}%)")

        # ä¿å­˜æ›´æ–°åçš„é…ç½®
        save_yaml_config(final_config, config_path)
        logger.info("ğŸ’¾ é…ç½®æ–‡ä»¶å·²æ›´æ–°")

        # ç¬¬äº”æ­¥ï¼šç”Ÿæˆè¾“å‡ºæ–‡ä»¶
        logger.info("\nğŸ“ ç¬¬äº”æ­¥ï¼šç”Ÿæˆè¾“å‡ºæ–‡ä»¶")
        logger.info("-" * 40)

        # å†™å…¥è®¢é˜…å­˜å‚¨æ–‡ä»¶
        sub_store_file = config_path.replace('.yaml', '_sub_store.txt')
        content = ("-- play_list --\n\n" + 
                  "\n".join(final_config["å¼€å¿ƒç©è€"]) + 
                  "\n\n-- sub_list --\n\n" + 
                  "\n".join(final_config["æœºåœºè®¢é˜…"]))
        with open(sub_store_file, 'w', encoding='utf-8') as f:
            f.write(content)
        logger.info(f"ğŸ“„ è®¢é˜…å­˜å‚¨æ–‡ä»¶å·²ä¿å­˜: {sub_store_file}")

        # ç¬¬å…­æ­¥ï¼šæ£€æµ‹èŠ‚ç‚¹æœ‰æ•ˆæ€§
        logger.info("\nğŸ” ç¬¬å…­æ­¥ï¼šæ£€æµ‹èŠ‚ç‚¹æœ‰æ•ˆæ€§")
        logger.info("-" * 40)

        loon_file = None
        clash_file = None
        v2_file = None

        # æ£€æµ‹æœºåœºè®¢é˜…èŠ‚ç‚¹
        if final_config["æœºåœºè®¢é˜…"]:
            valid_loon = await check_nodes(final_config["æœºåœºè®¢é˜…"], "loon", session)
            loon_file = config_path.replace('.yaml', '_loon.txt')
            write_url_list(valid_loon, loon_file)

        # æ£€æµ‹clashè®¢é˜…èŠ‚ç‚¹
        if final_config["clashè®¢é˜…"]:
            valid_clash = await check_nodes(final_config["clashè®¢é˜…"], "clash", session)
            clash_file = config_path.replace('.yaml', '_clash.txt')
            write_url_list(valid_clash, clash_file)

        # æ£€æµ‹v2è®¢é˜…èŠ‚ç‚¹
        if final_config["v2è®¢é˜…"]:
            valid_v2 = await check_nodes(final_config["v2è®¢é˜…"], "v2ray", session)
            v2_file = config_path.replace('.yaml', '_v2.txt')
            write_url_list(valid_v2, v2_file)

        # ç¬¬ä¸ƒæ­¥ï¼šåˆå¹¶æ–‡ä»¶åˆ° all.txt
        logger.info("\nğŸ”— ç¬¬ä¸ƒæ­¥ï¼šåˆå¹¶æ–‡ä»¶åˆ° all.txt")
        logger.info("-" * 40)
        all_file = config_path.replace('.yaml', '_all.txt')
        merge_files_to_all_txt(sub_store_file, loon_file, clash_file, v2_file, all_file)

        # ç¬¬å…«æ­¥ï¼šä¸‹è½½ all.txt ä¸­çš„è®¢é˜…åˆ° sub/ æ–‡ä»¶å¤¹ï¼ŒæŒ‰ç±»å‹åˆ†ç±»
        logger.info("\nğŸ“¥ ç¬¬å…«æ­¥ï¼šä¸‹è½½å¹¶åˆ†ç±»è®¢é˜…é“¾æ¥åˆ° sub/ æ–‡ä»¶å¤¹")
        logger.info("-" * 40)
        try:
            await download_and_process_all_txt(all_file)
        except Exception as e:
            logger.error(f"Error in download_and_process_all_txt: {e}")
            logger.info("ç»§ç»­æµç¨‹ï¼Œå°½ç®¡ä¸‹è½½æ­¥éª¤å¤±è´¥")

    logger.info("\nğŸ‰ è®¢é˜…ç®¡ç†æµç¨‹å®Œæˆï¼")
    logger.info("=" * 60)




if __name__ == '__main__':
    asyncio.run(main())